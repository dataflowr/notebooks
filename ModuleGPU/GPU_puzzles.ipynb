{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57624bc4-fb33-40cf-bdf3-5aa25f89d22f",
   "metadata": {},
   "source": "# Programming on GPUs\n\nThis notebook teaches the basics of GPU programming through hands-on exercises. We start with [Numba](https://numba.pydata.org/), a just-in-time compiler for Python that provides low-level GPU control, then move to [Triton](https://openai.com/index/triton/), OpenAI's high-level Python-like GPU programming language.\n\n**Learning approach:** This notebook emphasizes interactive coding with minimal upfront theory. You'll learn by doing. If you get stuck, ask for hints from your favorite chat assistant without requesting the complete solution.\n\n**Sources:** \n- Nvidia [CUDA Programming Guide](https://docs.nvidia.com/cuda/cuda-programming-guide/01-introduction/programming-model.html)\n- [GPU-Puzzles](https://github.com/srush/GPU-Puzzles) by Sasha Rush"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e256bcb-6387-4811-9959-f448426b42ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "import numpy as np\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cd6e1d-a546-4cef-b9a4-d0f42e24ba3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    action=\"ignore\", category=numba.NumbaPerformanceWarning, module=\"numba\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e929cee2-ac79-4576-9c9c-4442ef7123ff",
   "metadata": {},
   "source": "## Core GPU Concepts\n\nBefore we start coding, let's understand the GPU's execution model:\n\n- **Streaming Multiprocessor (SM)**: The GPU's computation unit (analogous to a CPU core)\n- **Thread**: The smallest unit of execution (processes one element)\n- **Thread Block (Block)**: A group of threads guaranteed to run on a single SM (can share memory and synchronize)\n- **Grid**: Thread blocks are organized into a 1D, 2D, or 3D grid\n- **Warp**: Within a thread block, threads are grouped into warps of 32 threads. All threads in a warp execute the same instruction simultaneously (SIMT: Single-Instruction Multiple-Threads)\n\n**Mental model:** Grid ‚Üí Blocks ‚Üí Threads ‚Üí Warps\n\nLet's start coding!"
  },
  {
   "cell_type": "markdown",
   "id": "876c52a6-60cd-4aa3-95bd-19bbdd1c65ff",
   "metadata": {},
   "source": "### Puzzle 1: Map\n\n**Goal:** Add 10 to each element of an array using parallel threads."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2911024-cc0b-46e9-9685-e1e6710f5920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_spec(a):\n",
    "    return a + 10\n",
    "\n",
    "# Size of our array\n",
    "SIZE = 4\n",
    "\n",
    "# Create input and output arrays\n",
    "a = np.arange(SIZE, dtype=np.float32)  # [0, 1, 2, 3]\n",
    "out = np.zeros(SIZE, dtype=np.float32)\n",
    "\n",
    "map_spec(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7924d20-47ab-4310-98fe-134781b5f651",
   "metadata": {},
   "source": "**Task:** Implement this using Numba so that each thread adds 10 to exactly one element of the array.\n\n**Hint:** Use `cuda.threadIdx.x` to get the current thread's index within its block."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a463ab-87e8-4a84-9c59-793297551b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CUDA kernel\n",
    "@cuda.jit\n",
    "def map_kernel(out, a):\n",
    "    # Get the thread index\n",
    "    i = cuda.threadIdx.x\n",
    "    # Each thread adds 10 to one element\n",
    "    # your code here\n",
    "\n",
    "\n",
    "# Copy arrays to GPU\n",
    "a_device = cuda.to_device(a)\n",
    "out_device = cuda.to_device(out)\n",
    "\n",
    "# kernel[grid, block](args)\n",
    "# Launch kernel: grid = 1 block, block = SIZE threads\n",
    "map_kernel[1, SIZE](out_device, a_device)\n",
    "\n",
    "# Copy result back to CPU\n",
    "result = out_device.copy_to_host()\n",
    "\n",
    "# Verify result\n",
    "expected = map_spec(a)\n",
    "print(f\"Input:    {a}\")\n",
    "print(f\"Output:   {result}\")\n",
    "print(f\"Expected: {expected}\")\n",
    "print(f\"Correct:  {np.allclose(result, expected)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16e2d63-6175-4c4b-95c2-c425a0804ce7",
   "metadata": {},
   "source": "### Puzzle 2: Vector Addition\n\n**Goal:** Add two vectors element-wise using parallel threads."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b8fcdf-cd27-4bb3-becd-60661a021407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zip_spec(a, b):\n",
    "    return a + b\n",
    "\n",
    "out = np.zeros(SIZE)\n",
    "a = np.arange(SIZE)\n",
    "b = np.arange(SIZE)\n",
    "zip_spec(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90295c2d-1514-46f5-a755-10cd76e7e421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CUDA kernel\n",
    "@cuda.jit\n",
    "def zip_kernel(out, a, b):\n",
    "    # Get the thread index\n",
    "    i = cuda.threadIdx.x\n",
    "    # your code here\n",
    "\n",
    "# A function to move vectors on device\n",
    "def init_pb(a=a, b=b, out=out):\n",
    "    a_device = cuda.to_device(a)\n",
    "    b_device = cuda.to_device(b)\n",
    "    out_device = cuda.to_device(out)\n",
    "    return a_device, b_device, out_device\n",
    "\n",
    "a_device, b_device, out_device = init_pb()\n",
    "\n",
    "# Launch kernel: 1 block, SIZE threads\n",
    "zip_kernel[1, SIZE](out_device, a_device, b_device)\n",
    "\n",
    "# Copy result back to CPU\n",
    "result = out_device.copy_to_host()\n",
    "\n",
    "# Verify result\n",
    "expected = zip_spec(a, b)\n",
    "print(f\"Input a:  {a}\")\n",
    "print(f\"Input b:  {b}\")\n",
    "print(f\"Output:   {result}\")\n",
    "print(f\"Expected: {expected}\")\n",
    "print(f\"Correct:  {np.allclose(result, expected)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b00ec4-6aa5-44e8-8a79-0e9bb0a6ac67",
   "metadata": {},
   "source": "**Experiment:** What happens if you launch more threads than the array size?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ed1ebc-96fd-4d7c-afc2-172f47b48840",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_device, b_device, out_device = init_pb()\n",
    "\n",
    "NUM_TRHEADS = 2*SIZE\n",
    "zip_kernel[1, NUM_TRHEADS](out_device, a_device, b_device)\n",
    "\n",
    "# Copy result back to CPU\n",
    "result = out_device.copy_to_host()\n",
    "\n",
    "# Verify result\n",
    "expected = zip_spec(a, b)\n",
    "print(f\"Output:   {result}\")\n",
    "print(f\"Expected: {expected}\")\n",
    "print(f\"Correct:  {np.allclose(result, expected)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b977472-73cf-49cf-a808-1b79ffce48d5",
   "metadata": {},
   "source": "**Result:** Still works but unsafe! Excess threads access out-of-bounds memory, which can cause crashes or silent data corruption.\n\n**Task:** Add a guard clause to prevent threads from accessing memory beyond the array bounds."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47d9b2f-d542-4286-aa78-19152ce85474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA kernel with Guard\n",
    "@cuda.jit\n",
    "def zip_guard_kernel(out, a, b, size):\n",
    "    # Get the thread index\n",
    "    i = cuda.threadIdx.x\n",
    "    # your code here\n",
    "\n",
    "a_device, b_device, out_device = init_pb()\n",
    "\n",
    "NUM_TRHEADS = 2*SIZE\n",
    "zip_guard_kernel[1, NUM_TRHEADS](out_device, a_device, b_device, SIZE)\n",
    "\n",
    "# Copy result back to CPU\n",
    "result = out_device.copy_to_host()\n",
    "\n",
    "# Verify result\n",
    "expected = zip_spec(a, b)\n",
    "print(f\"Output:   {result}\")\n",
    "print(f\"Expected: {expected}\")\n",
    "print(f\"Correct:  {np.allclose(result, expected)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adb57cf-877f-4c2b-839a-ebf1f5ef877f",
   "metadata": {},
   "source": "### Puzzle 3: 2D Matrices\n\n**Goal:** Apply the map operation to a 2D matrix using 2D thread blocks."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35e349e-0d40-43a0-b409-79ad7836d298",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(SIZE * SIZE).reshape((SIZE, SIZE))\n",
    "out = map_spec(a)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40147ee-56f6-4479-9eb1-58e8327fa17c",
   "metadata": {},
   "source": "**Key insight:** Thread blocks can be organized in 2D or 3D shapes, which simplifies mapping threads to 2D/3D data structures.\n\n**Task:** Use a 2D thread block where each thread handles one matrix element.\n\n**Hint:** Use `cuda.threadIdx.x` and `cuda.threadIdx.y` to get both coordinates."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948815aa-6a9e-428d-b472-f9858ef1dcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def map_2d_kernel(out, a, size):\n",
    "    i = cuda.threadIdx.x\n",
    "    j = cuda.threadIdx.y\n",
    "    # your code here\n",
    "\n",
    "a_device, b_device, out_device = init_pb(a=a, out=np.zeros_like(out))\n",
    "\n",
    "TRHEAD_BLOCK = (SIZE, SIZE)\n",
    "map_2d_kernel[1, TRHEAD_BLOCK](out_device, a_device, SIZE)\n",
    "\n",
    "result = out_device.copy_to_host()\n",
    "\n",
    "# Verify result\n",
    "expected = map_spec(a)\n",
    "print(f\"Output:   {result}\")\n",
    "print(f\"Expected: {expected}\")\n",
    "print(f\"Correct:  {np.allclose(result, expected)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9525d76-f77e-4d89-9a36-9081e2f4f135",
   "metadata": {},
   "source": "### Puzzle 4: Broadcasting\n\n**Goal:** Add two vectors with broadcasting (column + row ‚Üí matrix)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bc22dc-5282-4687-a058-c14441863971",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(SIZE).reshape(SIZE, 1)\n",
    "b = np.arange(SIZE).reshape(1, SIZE)\n",
    "out = a + b\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858dcc2a-20ed-472c-8125-7621ea5c8c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def broadcast_kernel(out, a, b, size):\n",
    "    i = cuda.threadIdx.x\n",
    "    j = cuda.threadIdx.y\n",
    "    # your code here\n",
    "    \n",
    "\n",
    "a_device, b_device, out_device = init_pb(a=a, b=b, out=np.zeros_like(out))\n",
    "\n",
    "THREAD_BLOCK = (2*SIZE, 3*SIZE)\n",
    "broadcast_kernel[1, THREAD_BLOCK](out_device, a_device, b_device, SIZE)\n",
    "\n",
    "result = out_device.copy_to_host()\n",
    "\n",
    "# Verify result\n",
    "expected = a + b\n",
    "print(f\"Output:   {result}\")\n",
    "print(f\"Expected: {expected}\")\n",
    "print(f\"Correct:  {np.allclose(result, expected)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc96aa9-bacd-4355-b86f-47d58c779687",
   "metadata": {},
   "source": "**New concept:** So far we've used only **thread blocks**. Now let's use the **grid** dimension.\n\nLike thread blocks, grids can be 1D, 2D, or 3D.\n\n**Task:** Use a 2D grid with a single thread per block to compute the broadcast addition.\n\n**Hint:** Use `cuda.blockIdx.x` and `cuda.blockIdx.y` to get the block's position in the grid."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed23d449-fcd6-44d1-a150-ea58779f1798",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def broadcast_grid_kernel(out, a, b, size):\n",
    "    i = cuda.blockIdx.x \n",
    "    j = cuda.blockIdx.y\n",
    "    # your code here\n",
    "\n",
    "\n",
    "a_device, b_device, out_device = init_pb(a=a, b=b, out=np.zeros_like(out))\n",
    "\n",
    "# 1 thread per block, 2D grid\n",
    "THREADS = 1\n",
    "GRID = (SIZE, SIZE)\n",
    "broadcast_grid_kernel[GRID, THREADS](out_device, a_device, b_device, SIZE)\n",
    "\n",
    "result = out_device.copy_to_host()\n",
    "\n",
    "# Verify result\n",
    "expected = a + b\n",
    "print(f\"Output:   {result}\")\n",
    "print(f\"Expected: {expected}\")\n",
    "print(f\"Correct:  {np.allclose(result, expected)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879131b7-cabf-47bc-ae67-0eedb3e1cba2",
   "metadata": {},
   "source": "**Next challenge:** Implement the same operation using a 1D grid and 1D thread blocks.\n\n**Hint:** You'll need to compute 2D indices (i, j) from 1D block and thread indices."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2281d6cd-4cc4-45f9-859c-e3dda4586a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def broadcast_grid_kernel(out, a, b, size):\n",
    "    # your code here\n",
    "    \n",
    "\n",
    "a_device, b_device, out_device = init_pb(a=a, b=b, out=np.zeros_like(out))\n",
    "\n",
    "THREADS = SIZE\n",
    "GRID = SIZE\n",
    "broadcast_grid_kernel[GRID, THREADS](out_device, a_device, b_device, SIZE)\n",
    "\n",
    "result = out_device.copy_to_host()\n",
    "\n",
    "# Verify result\n",
    "expected = a + b\n",
    "print(f\"Output:   {result}\")\n",
    "print(f\"Expected: {expected}\")\n",
    "print(f\"Correct:  {np.allclose(result, expected)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71de175-064f-4db6-9f9b-2ec9e397894d",
   "metadata": {},
   "source": "**Final challenge:** Use a 2D grid with 2D thread blocks.\n\nHere we configure `cuda.blockDim.x = cuda.blockDim.y = 2` (each block has 2√ó2=4 threads) and `SIZE//2 = 2` (we have 2√ó2=4 blocks).\n\n**Understanding the indexing:** Each element (i, j) in the output is computed by combining block and thread indices:\n\n| blockIdx.x | blockIdx.y | threadIdx.x | threadIdx.y | **i** | **j** | Computes |\n|------------|------------|-------------|-------------|-------|-------|----------|\n| 0 | 0 | 0 | 0 | **0** | **0** | out[0,0] |\n| 0 | 0 | 1 | 0 | **1** | **0** | out[1,0] |\n| 0 | 0 | 0 | 1 | **0** | **1** | out[0,1] |\n| 0 | 0 | 1 | 1 | **1** | **1** | out[1,1] |\n| 1 | 0 | 0 | 0 | **2** | **0** | out[2,0] |\n| 1 | 0 | 1 | 0 | **3** | **0** | out[3,0] |\n| 0 | 1 | 0 | 0 | **0** | **2** | out[0,2] |\n| 0 | 1 | 1 | 1 | **1** | **3** | out[1,3] |\n| 1 | 1 | 0 | 0 | **2** | **2** | out[2,2] |\n| 1 | 1 | 1 | 1 | **3** | **3** | out[3,3] |\n\n**Formula:** `i = blockIdx.x * blockDim.x + threadIdx.x` (similarly for j)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea2afc6-c4c4-4276-a79e-190ade51d2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def broadcast_grid_kernel(out, a, b, size):\n",
    "    # your code here\n",
    "    \n",
    "\n",
    "a_device, b_device, out_device = init_pb(a=a, b=b, out=np.zeros_like(out))\n",
    "\n",
    "THREADS = (SIZE//2 , SIZE//2)\n",
    "GRID = (SIZE//2, SIZE//2)  \n",
    "broadcast_grid_kernel[GRID, THREADS](out_device, a_device, b_device, SIZE)\n",
    "\n",
    "result = out_device.copy_to_host()\n",
    "\n",
    "# Verify result\n",
    "expected = a + b\n",
    "print(f\"Output:   {result}\")\n",
    "print(f\"Expected: {expected}\")\n",
    "print(f\"Correct:  {np.allclose(result, expected)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926e0e2a-602b-4d0b-b4b0-abef682330dc",
   "metadata": {},
   "source": "## GPU Memory Hierarchy\n\n<div>\n<img src=\"https://docs.nvidia.com/cuda/cuda-programming-guide/_images/gpu-cpu-system-diagram.png\" width=\"700\"/>\n</div>\n\n**Key memory types:**\n\n- **Global Memory (DRAM)**: Large but slow (~100s of cycles latency)\n  - Accessible to all SMs in the GPU\n  - System memory (host DRAM) is even slower (PCIe transfer required)\n  \n- **Shared Memory**: Small but fast (~1 cycle latency)\n  - On-chip memory shared by threads within a block\n  - Programmer-managed cache (you control what gets loaded)\n  - Limited size (typically 48-164 KB per SM)\n  \n- **Registers**: Fastest, private to each thread\n  - Directly accessible by thread (no load/store needed)\n  - Very limited (register spilling causes performance degradation)\n\n**Performance strategy:** Minimize global memory accesses by using shared memory as a manually-managed cache."
  },
  {
   "cell_type": "markdown",
   "id": "c30c674f-e7f1-4b4d-a7b2-aeb7b4d285c4",
   "metadata": {},
   "source": "### Puzzle 5: Pooling (Sliding Window Sum)\n\n**Goal:** Compute a sliding window sum where each output element is the sum of up to 3 input elements (current and 2 previous)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7633ae72-85e7-4196-bde1-fd187b750b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_spec(a):\n",
    "    out = np.zeros(a.shape)\n",
    "    for i in range(a.shape[0]):\n",
    "        out[i] = a[max(i - 2, 0) : i + 1].sum()\n",
    "    return out\n",
    "\n",
    "SIZE = 8\n",
    "a = np.arange(SIZE)\n",
    "out = pool_spec(a)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac15df9-9dcd-4493-bf31-571f774801a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def pool_kernel(out, a, size):\n",
    "    i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
    "    if i < size:\n",
    "        # Manually compute sum - can't use slicing in CUDA!\n",
    "        temp_sum = 0.0\n",
    "        for k in range(max(i - 2, 0), i + 1):\n",
    "            temp_sum += a[k]  # Use global memory\n",
    "        out[i] = temp_sum\n",
    "\n",
    "a_device, b_device, out_device = init_pb(a=a, out=np.zeros_like(out))\n",
    "\n",
    "THREADS = SIZE//2\n",
    "GRID = (2,1)  \n",
    "pool_kernel[GRID, THREADS](out_device, a_device, SIZE)\n",
    "\n",
    "result = out_device.copy_to_host()\n",
    "\n",
    "# Verify result\n",
    "expected = pool_spec(a)\n",
    "print(f\"Output:   {result}\")\n",
    "print(f\"Expected: {expected}\")\n",
    "print(f\"Correct:  {np.allclose(result, expected)}\")\n",
    "r = 3\n",
    "print(f\"number of access to global memory: 1 + 2 + {THREADS-2} threads x {r} reads = {1+2+(THREADS-2)*r} global reads per block -> {2*(1+2+(THREADS-2)*r)} global reads in total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9017cc-7569-48d5-89e0-fc931f89126a",
   "metadata": {},
   "source": "**Problem:** The naive implementation accesses global memory many times (12 reads for SIZE=8).\n\n**Solution:** Use shared memory to reduce global memory accesses to just 12 reads total (vs. many more with naive approach).\n\n**Memory hierarchy visualization:**\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  Global Memory (a, out)             ‚îÇ  ‚Üê Slow, accessible to ALL threads\n‚îÇ  - High latency (~100s cycles)      ‚îÇ\n‚îÇ  - Large capacity (GB)              ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n         ‚Üì                    ‚Üì\n   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n   ‚îÇ Block 0 ‚îÇ          ‚îÇ Block 1 ‚îÇ\n   ‚îÇ Shared  ‚îÇ          ‚îÇ Shared  ‚îÇ      ‚Üê Fast, accessible only within block\n   ‚îÇ Memory  ‚îÇ          ‚îÇ Memory  ‚îÇ         (~1 cycle)\n   ‚îÇ (fast)  ‚îÇ          ‚îÇ (fast)  ‚îÇ\n   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n**How it works - Halo Loading:**\n```\nGlobal:      [0, 1, 2, 3, 4, 5, 6, 7]\nBlock 0 loads:              Block 1 loads:\n        ‚Üì                          ‚Üì\nShared: [0, 0, 0, 1, 2, 3] Shared: [2, 3, 4, 5, 6, 7]\n         ‚îî‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n        halo    main data          halo    main data\n        (boundary padding)         (overlap with Block 0)\n```\n\n**Key constraints:**\n1. Shared memory size must be a **compile-time constant** (not runtime variable)\n2. After loading shared memory, call `cuda.syncthreads()` to ensure all threads see the data before using it\n\n**Task:** Implement the pooling kernel using shared memory with halo zones."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a76dde-423b-459d-8027-4eb99c3a0547",
   "metadata": {},
   "outputs": [],
   "source": [
    "TPB = 4  # Threads per block\n",
    "SharedMem = TPB + 2 # cannot be computed at runtime\n",
    "@cuda.jit\n",
    "def pool_kernel_shared(out, a, size):\n",
    "    # Allocate shared memory with HALO (extra elements for boundary)\n",
    "    # Need TPB + 2 extra elements (for the 2-element lookback)\n",
    "    shared = cuda.shared.array(SharedMem, numba.float32)\n",
    "    # your code here\n",
    "\n",
    "a_device, b_device, out_device = init_pb(a=a, out=np.zeros_like(out))\n",
    "\n",
    "\n",
    "GRID = (SIZE // TPB, 1)  # (2, 1) for SIZE=8, TPB=4\n",
    "pool_kernel_shared[GRID, TPB](out_device, a_device, SIZE)\n",
    "\n",
    "result = out_device.copy_to_host()\n",
    "\n",
    "expected = pool_spec(a)\n",
    "print(f\"Output:   {result}\")\n",
    "print(f\"Expected: {expected}\")\n",
    "print(f\"Correct:  {np.allclose(result, expected)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad89325-3bfe-4bd0-8905-ef6b43beccb8",
   "metadata": {},
   "source": "### Puzzle 6: Dot Product (Parallel Reduction)\n\n**Goal:** Compute dot product of two vectors: `dot(a, b) = sum(a[i] * b[i])`\n\n**Challenge:** Serial code is trivial, but parallel reduction is complex because we need to **combine results from multiple threads**."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce013ae9-bb0e-42e5-a36e-2f3e30c28da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_spec(a,b):\n",
    "    tot = 0\n",
    "    for i in range(len(a)):\n",
    "        tot += a[i]*b[i]\n",
    "    return tot\n",
    "\n",
    "SIZE = 8\n",
    "a = np.arange(SIZE, dtype=np.float32)\n",
    "b = np.arange(SIZE, dtype=np.float32)\n",
    "dot_spec(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0c68f3-7961-4ed5-ba8a-db7ea7a5e8a2",
   "metadata": {},
   "source": "**Solution approach:** Tree-based reduction within each block, then atomic addition across blocks.\n\n**Visual - Tree Reduction:**\n\n![Tree-based parallel sum](https://www.cs.uaf.edu/2012/fall/cs441/lecture/tree_sum_16td.png)\n\n**How it works:**\n1. Each thread computes one element-wise product: `a[i] * b[i]`\n2. Store results in shared memory\n3. Reduce in log2(n) steps: stride = n/2, n/4, n/8, ..., 1\n4. Thread 0 writes block's partial sum to output"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d852a2b3-62a3-4bbb-b0e8-98d0900447bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_tree(a,b):\n",
    "    size = len(a)\n",
    "    shared_mem = np.zeros(size)\n",
    "    for i in range(size):\n",
    "       shared_mem[i] = a[i]*b[i]\n",
    "    stride = size // 2\n",
    "    while stride > 0:\n",
    "        for i in range(stride):\n",
    "            shared_mem[i] += shared_mem[i+stride]\n",
    "        stride //=2\n",
    "    return shared_mem[0]\n",
    "dot_tree(a,b)       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab60ed8-d6d8-4fb8-ab1d-eb680df8a981",
   "metadata": {},
   "source": "**Task:** Implement tree-based dot product in Numba with the following configuration:\n- **256 threads per block** (fixed)\n- **Multiple blocks** to cover input size (calculated automatically)\n\n**Hint:** After the within-block reduction, use `cuda.atomic.add()` to safely accumulate partial sums from all blocks."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a757d9-7dac-44ae-a18d-ab3a01090926",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 800\n",
    "a = np.arange(SIZE, dtype=np.float32)\n",
    "b = np.arange(SIZE, dtype=np.float32)\n",
    "\n",
    "threads_per_block = 256\n",
    "blocks_per_grid = (SIZE + threads_per_block - 1) // threads_per_block\n",
    "print(f\"threads per block: {threads_per_block}\")\n",
    "print(f\"blocks per grid: {blocks_per_grid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903affae-6e73-441f-a126-5fa6d42fa1d6",
   "metadata": {},
   "source": [
    "For each block, you can implement the tree-based summation algorithm by first creating a shared memory of size 256 containing the `a[i]*b[i]` with the `i` associated witht the block thread. Then suming it accross the block. The last step consists in adding all the intermediate results: each block adds its result. For this last step, you might want to use `cuda.atomic.add` see below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaffa23-43cd-4bcf-aa9d-d291dc1f0df5",
   "metadata": {},
   "source": "### Understanding Atomic Operations\n\n**Why do we need atomics?**\n\nWhen multiple threads write to the same memory location, non-atomic operations can lose updates due to **race conditions**.\n\n**The Problem Without Atomics:**\n\nWhen you write `out[0] = out[0] + value`, it's actually 3 separate steps:\n```python\n# out[0] = out[0] + value breaks down to:\n1. READ:   temp = out[0]      # Read current value\n2. MODIFY: temp = temp + value # Add to it\n3. WRITE:  out[0] = temp       # Write back\n```\n\n**Race condition example:**\n```\nInitial: out[0] = 0\n\nThread A (Block 0):              Thread B (Block 1):\n1. READ: temp_A = 0             \n2. MODIFY: temp_A = 0 + 5       \n                                 1. READ: temp_B = 0      ‚Üê Still sees 0!\n3. WRITE: out[0] = 5            \n                                 2. MODIFY: temp_B = 0 + 3 ‚Üê Uses old value!\n                                 3. WRITE: out[0] = 3      ‚Üê Overwrites 5!\n\nFinal: out[0] = 3  ‚ùå Should be 8!\n```\n\n**What Atomics Do:**\n\n`cuda.atomic.add(out, 0, value)` **locks the memory location** during the entire read-modify-write:\n\n```\nInitial: out[0] = 0\n\nThread A (Block 0):              Thread B (Block 1):\nüîí LOCK out[0]\n1. READ: temp_A = 0             \n2. MODIFY: temp_A = 0 + 5       \n3. WRITE: out[0] = 5            \nüîì UNLOCK out[0]\n                                 üîí LOCK out[0]  ‚Üê Must wait for unlock\n                                 1. READ: temp_B = 5      ‚Üê Sees updated value!\n                                 2. MODIFY: temp_B = 5 + 3\n                                 3. WRITE: out[0] = 8\n                                 üîì UNLOCK out[0]\n\nFinal: out[0] = 8  ‚úÖ Correct!\n```\n\n**Key takeaway:** Use atomics when multiple threads update the same memory location."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6919a7-95f1-4887-b86d-667784b92405",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def dot_kernel_numba(a, b, out, size):\n",
    "    shared = cuda.shared.array(256, numba.float32)\n",
    "    # your code here\n",
    "    \n",
    "\n",
    "expected = np.dot(a, b)\n",
    "a_device, b_device, out_device = init_pb(a=a, b=b, out=np.zeros_like([expected]))\n",
    "\n",
    "size = a_device.shape[0]\n",
    "dot_kernel_numba[blocks_per_grid, threads_per_block](a_device, b_device, out_device, size)\n",
    "\n",
    "result = out_device.copy_to_host()\n",
    "print(f\"CUDA result: {result[0]}\")\n",
    "print(f\"NumPy result: {expected}\")\n",
    "print(f\"Match: {np.allclose(result[0], expected)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659e6827-ddc8-4e6a-8ae6-1f291f089b3b",
   "metadata": {},
   "source": "## Numba vs Triton: Conceptual Comparison\n\n### Numba CUDA: Grid and Block Dimensions\n\n**Key concepts:**\n- `kernel[grid, block](args)` - Launch syntax\n- **Grid** = `(blocks_x, blocks_y, blocks_z)` - How many blocks\n- **Block** = `(threads_x, threads_y, threads_z)` - Threads per block\n- **Total threads** = `grid_x √ó grid_y √ó grid_z √ó block_x √ó block_y √ó block_z`\n- **Manual indexing**: You compute indices using `blockIdx`, `threadIdx`, `blockDim`"
  },
  {
   "cell_type": "markdown",
   "id": "c24c0585-aae0-465c-9b94-36dac9aaf0db",
   "metadata": {},
   "source": "### Triton: Program Grid (Higher Abstraction)\n\nIn **Triton**, you specify a **program grid** and work with **program IDs**. Triton handles the low-level threading automatically.\n\n**Key concepts:**\n- `kernel[grid](args, BLOCK_SIZE=...)` - Launch syntax\n- **Grid** = `(programs_x, programs_y, programs_z)` - Number of program instances\n- **No explicit thread dimensions** - Triton vectorizes automatically\n- **Work with blocks of data** using `tl.arange()` and vectorized operations\n\n---\n\n### Comparison Table\n\n| Aspect | Numba CUDA | Triton |\n|--------|------------|--------|\n| **Launch syntax** | `kernel[grid, block](args)` | `kernel[grid](args, BLOCK=...)` |\n| **Grid represents** | Number of **blocks** | Number of **programs** |\n| **Block/Thread control** | Explicit: `(tx, ty, tz)` per block | Abstracted: work on data blocks |\n| **Thread indexing** | Manual: `blockIdx`, `threadIdx` | Automatic: `tl.program_id()` + `tl.arange()` |\n| **Typical grid** | `(n_blocks_x, n_blocks_y, n_blocks_z)` | `(n_programs_x, n_programs_y, n_programs_z)` |\n| **Typical block** | `(threads_x, threads_y, threads_z)` | N/A (implicit in `BLOCK_SIZE`) |\n| **Memory access** | Per-thread scalar indexing | Vectorized block operations |\n| **Abstraction level** | Low-level (like CUDA C) | High-level (compiler optimizes) |\n| **Synchronization** | Explicit: `cuda.syncthreads()` | Mostly automatic |\n\n---\n\n### Key Takeaway\n\n- **Numba CUDA**: You think in terms of **blocks of threads** (2-level hierarchy: grid ‚Üí blocks ‚Üí threads)\n- **Triton**: You think in terms of **programs operating on data blocks** (1-level: grid ‚Üí programs, with automatic vectorization)\n\n**Mental model:** Each Triton program ‚âà one CUDA block, but Triton auto-vectorizes the thread-level work.\n\n---\n\n### Practical Example: Vector Addition\n\nBelow is the solution from an earlier Numba puzzle:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342ddee6-0796-493d-99ea-93a44ed3bc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA kernel with Guard\n",
    "@cuda.jit\n",
    "def zip_guard_kernel(out, a, b, size):\n",
    "    i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
    "    if i < size:\n",
    "        out[i] = a[i] + b[i]\n",
    "\n",
    "SIZE = 1000\n",
    "out = np.zeros(SIZE)\n",
    "a = np.arange(SIZE)\n",
    "b = np.arange(SIZE)\n",
    "\n",
    "a_device, b_device, out_device = init_pb(a=a, b=b, out=out)\n",
    "\n",
    "threads_per_block = 256\n",
    "blocks_per_grid = (SIZE + threads_per_block - 1) // threads_per_block\n",
    "zip_guard_kernel[blocks_per_grid, threads_per_block](out_device, a_device, b_device, SIZE)\n",
    "\n",
    "# Copy result back to CPU\n",
    "result = out_device.copy_to_host()\n",
    "\n",
    "# Verify result\n",
    "expected = zip_spec(a, b)\n",
    "print(f\"Correct:  {np.allclose(result, expected)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be358d06-3d3a-41c3-ac03-46f5f610e70d",
   "metadata": {},
   "source": "### Triton Implementation: Vector Addition\n\n**Key differences from Numba:**\n- Works with PyTorch tensors (no manual memory management)\n- Processes multiple elements per program (vectorized)\n- `BLOCK_SIZE` is a compile-time constant for optimization"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830121a7-ecf5-4272-8737-3c9ac30b79df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "import torch\n",
    "from einops import rearrange\n",
    "\n",
    "def get_device(index: int = 0) -> torch.device:\n",
    "    \"\"\"Try to use the GPU if possible, otherwise, use CPU.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(f\"cuda:{index}\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1173350e-99b7-4896-9106-d2dff7765435",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def zip_guard_triton(a_ptr, b_ptr, out_ptr, n, BLOCK_SIZE: tl.constexpr):\n",
    "    # Triton uses program_id (block index) instead of explicit blockIdx/threadIdx\n",
    "    # Each \"program\" processes BLOCK_SIZE elements at once (vectorized)\n",
    "    pid = tl.program_id(0)\n",
    "    \n",
    "    # Triton computes offsets for a *vector* of BLOCK_SIZE elements\n",
    "    # Unlike Numba where each thread processes 1 element,\n",
    "    # Triton processes multiple elements per program instance\n",
    "    offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
    "    \n",
    "    # Triton uses mask-based guards for vectorized operations\n",
    "    # instead of scalar if-statements (if i < size)\n",
    "    mask = offset < n\n",
    "    \n",
    "    # Vectorized load: loads BLOCK_SIZE elements at once with mask\n",
    "    # Numba loads scalar: a[i]\n",
    "    a = tl.load(a_ptr + offset, mask=mask)\n",
    "    b = tl.load(b_ptr + offset, mask=mask)\n",
    "    \n",
    "    # Vectorized computation (same as Numba but on vectors)\n",
    "    c = a + b\n",
    "    \n",
    "    # Vectorized store with mask (vs. scalar store in Numba)\n",
    "    tl.store(out_ptr + offset, c, mask=mask)\n",
    "\n",
    "\n",
    "# Triton works directly with PyTorch tensors (no manual copy_to_host)\n",
    "# Numba requires explicit device memory management (init_pb, copy_to_host)\n",
    "a = torch.randn(SIZE, device=get_device())\n",
    "b = torch.randn(SIZE, device=get_device())\n",
    "out = torch.empty_like(a)\n",
    "\n",
    "\n",
    "# Launch syntax differences:\n",
    "# - BLOCK_SIZE is a compile-time constant (tl.constexpr) for optimization\n",
    "# - Only need to specify grid dimensions (not threads_per_block)\n",
    "# - Triton auto-vectorizes within each program\n",
    "BLOCK_SIZE = 256\n",
    "grid = (triton.cdiv(SIZE, BLOCK_SIZE),)  # Only grid size, not block size\n",
    "zip_guard_triton[grid](a, b, out, SIZE, BLOCK_SIZE=BLOCK_SIZE)\n",
    "\n",
    "expected = zip_spec(a, b)\n",
    "print(f\"Correct:  {np.allclose(out.cpu().numpy(), expected.cpu().numpy())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0b1fb5-ff7a-4730-8283-5c8beb9ef55b",
   "metadata": {},
   "source": "### Triton Dot Product\n\n**Task:** Implement dot product in Triton.\n\n**Useful functions:**\n- [`tl.sum()`](https://triton-lang.org/main/python-api/generated/triton.language.sum.html) - Parallel reduction within a block\n- [`tl.atomic_add()`](https://triton-lang.org/main/python-api/generated/triton.language.atomic_add.html) - Atomic addition across programs"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a72ef4-0b17-47e6-90e6-837363b56613",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def dot_kernel(\n",
    "    a_ptr,      # Pointer to first input vector\n",
    "    b_ptr,      # Pointer to second input vector  \n",
    "    out_ptr,    # Pointer to output scalar\n",
    "    size,       # Size of vectors\n",
    "    BLOCK_SIZE: tl.constexpr,  # Elements per program\n",
    "):\n",
    "    # Program ID (analogous to blockIdx.x)\n",
    "    pid = tl.program_id(0)\n",
    "    \n",
    "    # Compute offsets for this program's block\n",
    "    block_start = pid * BLOCK_SIZE\n",
    "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "    \n",
    "    # Mask for boundary handling\n",
    "    mask = offsets < size\n",
    "    \n",
    "    # Load data \n",
    "    a = tl.load(a_ptr + offsets, mask=mask)\n",
    "    b = tl.load(b_ptr + offsets, mask=mask)\n",
    "    \n",
    "    # Element-wise multiplication\n",
    "    products = a * b\n",
    "    \n",
    "    # Reduce sum within this block (automatic parallel reduction!)\n",
    "    block_sum = tl.sum(products)\n",
    "    \n",
    "    # Atomic add to output (single thread per block does this)\n",
    "    tl.atomic_add(out_ptr, block_sum)\n",
    "\n",
    "\n",
    "def dot_triton(a, b):\n",
    "    \"\"\"Wrapper function to launch the kernel\"\"\"\n",
    "    # Allocate output\n",
    "    out = torch.zeros(1, device=a.device, dtype=a.dtype)\n",
    "    \n",
    "    # Grid and block configuration\n",
    "    size = a.shape[0]\n",
    "    BLOCK_SIZE = 256  \n",
    "    grid = (triton.cdiv(size, BLOCK_SIZE),)\n",
    "    \n",
    "    # Launch kernel\n",
    "    dot_kernel[grid](a, b, out, size, BLOCK_SIZE=BLOCK_SIZE)\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "# Usage example\n",
    "SIZE = 10\n",
    "a = torch.arange(SIZE, dtype=torch.float32, device=get_device())\n",
    "b = torch.arange(SIZE, dtype=torch.float32, device=get_device())\n",
    "\n",
    "result_triton = dot_triton(a, b)\n",
    "result_torch = torch.dot(a, b)\n",
    "\n",
    "print(f\"Triton result: {result_triton.item()}\")\n",
    "print(f\"PyTorch result: {result_torch.item()}\")\n",
    "print(f\"Match: {torch.allclose(result_triton, result_torch)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ff070e-1cd8-47f7-9748-afb130d1681a",
   "metadata": {},
   "source": "### Triton Softmax\n\n**Goal:** Implement softmax for a batch of vectors `z` of shape `(batch_size, dim)`: `torch.softmax(z, dim=1)`\n\n**Key concept - Tensor Strides:**\n\nStrides define how to navigate multi-dimensional tensors in contiguous memory.\n\n**Visual:**\n```\nMemory: [a, b, c, d, e, f, g, h, i, j, k, l]\nShape (3, 4), stride (4, 1):\n  [[a, b, c, d],    ‚Üê skip 4 for next row, skip 1 for next col\n   [e, f, g, h],\n   [i, j, k, l]]\n```\n\n**Why it matters:** To process one row in softmax, we need to:\n1. Find the starting address: `row_start_ptr = base_ptr + row_idx * row_stride`\n2. Load all elements in that row using column stride"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3b901c-56ee-4236-b561-93a2e81d80e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contiguous 2D tensor (3√ó4)\n",
    "x = torch.randn(3, 4)\n",
    "x.stride()  # (4, 1)\n",
    "# - Move to next row: skip 4 elements\n",
    "# - Move to next column: skip 1 element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9c0f44-1ac9-43a3-b9ef-9d21e6b895a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transposed (now 4√ó3)\n",
    "y = x.t()\n",
    "y.stride()  # (1, 4)\n",
    "# - Move to next row: skip 1 element (was column)\n",
    "# - Move to next column: skip 4 elements (was row)\n",
    "# Note: y shares memory with x, just different access pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476e4efb-5ddc-4b65-87a4-b647a6cba214",
   "metadata": {},
   "source": "**Implementation strategy:**\n- Each program processes one complete row independently\n- Each program computes softmax over all columns in its row\n- Use row_stride to locate the starting address for each row"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380315a5-3677-4a59-be40-45f942bc6dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def triton_softmax_kernel(x_ptr, y_ptr, x_row_stride, y_row_stride, num_cols, BLOCK_SIZE: tl.constexpr):\n",
    "    assert num_cols <= BLOCK_SIZE\n",
    "    # Process each row independently\n",
    "    # your code here\n",
    "\n",
    "def triton_softmax(x: torch.Tensor):\n",
    "    x = x.contiguous()\n",
    "    # Allocate output tensor\n",
    "    y = torch.empty_like(x)\n",
    "    # Determine grid\n",
    "    M, N = x.shape                          # Number of rows x number of columns\n",
    "    block_size = triton.next_power_of_2(N)  # Each block contains all the columns\n",
    "    num_blocks = M                          # Each block is a row\n",
    "    # Launch kernel\n",
    "    triton_softmax_kernel[(M,)](\n",
    "        x_ptr=x, y_ptr=y,\n",
    "        x_row_stride=x.stride(0), y_row_stride=y.stride(0),\n",
    "        num_cols=N, BLOCK_SIZE=block_size\n",
    "    )\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1735d03-3bfa-41b5-8b4c-67235974d8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "x = torch.randn(1823, 781, device=get_device())\n",
    "y_triton = triton_softmax(x)\n",
    "y_torch = torch.softmax(x, axis=1)\n",
    "assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89635810-32e9-4198-8a3e-fb168eed7aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_triton = triton_softmax(x.t())\n",
    "y_torch = torch.softmax(x.t(), axis=1)\n",
    "assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc34a21-9928-44d1-820b-4fdb1518a250",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = get_device()\n",
    "\n",
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['N'],  # argument names to use as an x-axis for the plot\n",
    "        x_vals=[128 * i for i in range(2, 100)],  # different possible values for `x_name`\n",
    "        line_arg='provider',  # argument name whose value corresponds to a different line in the plot\n",
    "        line_vals=['triton', 'torch'],  # possible values for `line_arg``\n",
    "        line_names=[\"Triton\", \"Torch\"],  # label name for the lines\n",
    "        styles=[('blue', '-'), ('green', '-')],  # line styles\n",
    "        ylabel=\"GB/s\",  # label name for the y-axis\n",
    "        plot_name=\"softmax-performance\",  # name for the plot. Used also as a file name for saving the plot.\n",
    "        args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`\n",
    "    ))\n",
    "\n",
    "def benchmark(M, N, provider):\n",
    "    x = torch.randn(M, N, device=DEVICE, dtype=torch.float32)\n",
    "    stream = getattr(torch, DEVICE.type).Stream()\n",
    "    getattr(torch, DEVICE.type).set_stream(stream)\n",
    "    if provider == 'torch':\n",
    "        ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))\n",
    "    if provider == 'triton':\n",
    "        ms = triton.testing.do_bench(lambda: triton_softmax(x))\n",
    "    gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)\n",
    "    return gbps(ms)\n",
    "\n",
    "\n",
    "benchmark.run(show_plots=True, print_data=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cefcdf-e2b0-4332-b661-caeae619b06b",
   "metadata": {},
   "source": "## Triton Block Pointers (Advanced)\n\n**Block pointers** are Triton's high-level abstraction for tiled memory access, eliminating error-prone manual pointer arithmetic.\n\n**Core concept:** Instead of computing `ptr + offset` manually, block pointers encapsulate:\n- **Where you are** in the tensor (offsets)\n- **What tile** you're accessing (block_shape)\n- **How to navigate** memory (strides)\n\n---\n\n### Example: 2D Tensor Block Pointer\n\n```python\nx_block_ptr = tl.make_block_ptr(\n    x_ptr,                                    # Base address\n    shape=(ROWS, D),                          # Full tensor: ROWS √ó D\n    strides=(x_stride_row, x_stride_dim),     # Jump between rows/cols\n    offsets=(row_tile_idx * ROWS_TILE_SIZE, 0),  # Start at row tile, col 0\n    block_shape=(ROWS_TILE_SIZE, D_TILE_SIZE),   # Tile size\n    order=(1, 0),                             # Row-major layout\n)\n```\n\n**Visual:**\n```\nFull tensor (ROWS √ó D):\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ [0,0]  ......  [0, D-1]         ‚îÇ ‚Üê row_tile_idx=0 loads ROWS_TILE_SIZE rows\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ [ROWS_TILE_SIZE, 0] ...         ‚îÇ ‚Üê row_tile_idx=1\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ  ...                            ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n    ‚îî‚îÄ D_TILE_SIZE ‚îÄ‚îò  (tile width)\n```\n\n---\n\n### Example: 1D Tensor Block Pointer\n\n```python\nweight_block_ptr = tl.make_block_ptr(\n    weight_ptr,\n    shape=(D,),                    # 1D vector\n    strides=(weight_stride_dim,),  # Element spacing\n    offsets=(0,),                  # Start at beginning\n    block_shape=(D_TILE_SIZE,),    # Load D_TILE_SIZE elements\n    order=(0,),                    # 1D ordering\n)\n```\n\n---\n\n### Usage Pattern: Tiled Computation\n\n```python\n# Initialize accumulator\noutput = tl.zeros((ROWS_TILE_SIZE,), dtype=tl.float32)\n\n# Loop over D dimension in tiles\nfor i in range(tl.cdiv(D, D_TILE_SIZE)):\n    # Load current tiles with automatic boundary checking\n    row = tl.load(x_block_ptr, boundary_check=(0, 1), padding_option=\"zero\")\n    # Shape: (ROWS_TILE_SIZE, D_TILE_SIZE)\n    \n    weight = tl.load(weight_block_ptr, boundary_check=(0,), padding_option=\"zero\")\n    # Shape: (D_TILE_SIZE,)\n    \n    # Compute weighted sum: sum over columns for each row\n    output += tl.sum(row * weight[None, :], axis=1)  # Accumulate\n    \n    # Advance to next tile\n    x_block_ptr = tl.advance(x_block_ptr, (0, D_TILE_SIZE))      # Move right\n    weight_block_ptr = tl.advance(weight_block_ptr, (D_TILE_SIZE,))\n\n# Write result\ntl.store(output_block_ptr, output, boundary_check=(0,))\n```\n\n---\n\n### Key Features\n\n1. **Automatic boundary checking:** `boundary_check=(0, 1)` handles tiles that don't fit perfectly\n   - Dimension 0 (rows): May not divide evenly by `ROWS_TILE_SIZE`\n   - Dimension 1 (cols): May not divide evenly by `D_TILE_SIZE`\n\n2. **Tiled computation:** Process large dimension in chunks, accumulating results\n\n3. **Clean navigation:** `.advance()` moves to next tile without manual offset math\n\n---\n\n### Comparison: Manual vs. Block Pointers\n\n```python\n# OLD WAY (manual pointer arithmetic):\nrow_offsets = row_tile_idx * ROWS_TILE_SIZE + tl.arange(0, ROWS_TILE_SIZE)\ncol_offsets = tl.arange(0, D_TILE_SIZE)\nx_ptrs = x_ptr + row_offsets[:, None] * x_stride_row + col_offsets[None, :] * x_stride_dim\nmask = (row_offsets < ROWS)[:, None] & (col_offsets < D)[None, :]\nrow = tl.load(x_ptrs, mask=mask)\n\n# NEW WAY (block pointers):\nx_block_ptr = tl.make_block_ptr(...)\nrow = tl.load(x_block_ptr, boundary_check=(0, 1))\n```\n\n**Benefits:**\n- Automatic bounds checking\n- Cleaner code with `.advance()`\n- Better compiler optimization\n- Correct handling of non-contiguous tensors\n\n---\n\nThe code below is adapted from the [Stanford CS336 course](https://github.com/stanford-cs336/assignment2-systems)."
  },
  {
   "cell_type": "markdown",
   "id": "81a2f632-baab-47d8-936d-9d42643c23a5",
   "metadata": {},
   "source": "### Integrating Triton Kernels with PyTorch\n\n**Example:** Weighted sum operation using block pointers for efficient tiled computation."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c03191b-645c-4c91-9423-f5ea74b3137d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_sum(x, weight):\n",
    "    # Here, assume that x has n-dim shape [..., D], and weight has 1D shape [D]\n",
    "    return (weight * x).sum(axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def weighted_sum_fwd(\n",
    "    x_ptr, weight_ptr,  # Input pointers\n",
    "    output_ptr,  # Output pointer\n",
    "    x_stride_row, x_stride_dim,  # Strides tell us how to move one element in each axis of a tensor\n",
    "    weight_stride_dim,  # Likely 1\n",
    "    output_stride_row,  # Likely 1\n",
    "    ROWS, D,\n",
    "    ROWS_TILE_SIZE: tl.constexpr, D_TILE_SIZE: tl.constexpr,  # Tile shapes must be known at compile time\n",
    "):\n",
    "    # Each instance will compute the weighted sum of a tile of rows of x.\n",
    "    # `tl.program_id` gives us a way to check which thread block we're running in\n",
    "    row_tile_idx = tl.program_id(0)\n",
    "    \n",
    "    # Block pointers give us a way to select from an ND region of memory\n",
    "    # and move our selection around.\n",
    "    # The block pointer must know:\n",
    "    # - The pointer to the first element of the tensor\n",
    "    # - The overall shape of the tensor to handle out-of-bounds access\n",
    "    # - The strides of each dimension to use the memory layout properly\n",
    "    # - The ND coordinates of the starting block, i.e., \"offsets\"\n",
    "    # - The block shape to use load/store at a time\n",
    "    # - The order of the dimensions in memory from major to minor\n",
    "    # axes (= np.argsort(strides)) for optimizations, especially useful on H100\n",
    "    \n",
    "    x_block_ptr = tl.make_block_ptr(\n",
    "        x_ptr,\n",
    "        shape=(ROWS, D),\n",
    "        strides=(x_stride_row, x_stride_dim),\n",
    "        offsets=(row_tile_idx * ROWS_TILE_SIZE, 0),\n",
    "        block_shape=(ROWS_TILE_SIZE, D_TILE_SIZE),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    \n",
    "    weight_block_ptr = tl.make_block_ptr(\n",
    "        weight_ptr,\n",
    "        shape=(D,),\n",
    "        strides=(weight_stride_dim,),\n",
    "        offsets=(0,),\n",
    "        block_shape=(D_TILE_SIZE,),\n",
    "        order=(0,),\n",
    "    )\n",
    "    \n",
    "    output_block_ptr = tl.make_block_ptr(\n",
    "        output_ptr,\n",
    "        shape=(ROWS,),\n",
    "        strides=(output_stride_row,),\n",
    "        offsets=(row_tile_idx * ROWS_TILE_SIZE,),\n",
    "        block_shape=(ROWS_TILE_SIZE,),\n",
    "        order=(0,),\n",
    "    )\n",
    "    \n",
    "    # Initialize a buffer to write to\n",
    "    output = tl.zeros((ROWS_TILE_SIZE,), dtype=tl.float32)\n",
    "    \n",
    "    for i in range(tl.cdiv(D, D_TILE_SIZE)):\n",
    "        # Load the current block pointer\n",
    "        # Since ROWS_TILE_SIZE might not divide ROWS, and D_TILE_SIZE might not divide D,\n",
    "        # we need boundary checks for both dimensions\n",
    "        row = tl.load(x_block_ptr, boundary_check=(0, 1), padding_option=\"zero\")  # (ROWS_TILE_SIZE, D_TILE_SIZE)\n",
    "        weight = tl.load(weight_block_ptr, boundary_check=(0,), padding_option=\"zero\")  # (D_TILE_SIZE,)\n",
    "        \n",
    "        # Compute the weighted sum of the row.\n",
    "        output += tl.sum(row * weight[None, :], axis=1)\n",
    "        \n",
    "        # Move the pointers to the next tile.\n",
    "        # These are (rows, columns) coordinate deltas\n",
    "        x_block_ptr = x_block_ptr.advance((0, D_TILE_SIZE))  # Move by D_TILE_SIZE in the last dimension\n",
    "        weight_block_ptr = weight_block_ptr.advance((D_TILE_SIZE,))  # Move by D_TILE_SIZE\n",
    "    \n",
    "    # Write output to the output block pointer (a single scalar per row).\n",
    "    # Since ROWS_TILE_SIZE might not divide ROWS, we need boundary checks\n",
    "    tl.store(output_block_ptr, output, boundary_check=(0,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1707e502-0d50-4ecb-94fb-bd5850e078e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_sum_triton(x: torch.Tensor, weight: torch.Tensor):\n",
    "    D = x.shape[-1]\n",
    "    output_dims = x.shape[:-1]   \n",
    "    # Reshape input tensor to 2D\n",
    "    \n",
    "    x = rearrange(x, \"... d -> (...) d\")\n",
    "    # Need to initialize empty result tensor. Note that these elements are not necessarily 0!\n",
    "    y = torch.empty(x.shape[0], device=x.device)\n",
    "\n",
    "    D_TILE_SIZE = triton.next_power_of_2(D) // 16  # Roughly 16 loops through the embedding dimension\n",
    "    ROWS_TILE_SIZE = 16  # Each thread processes 16 batch elements at a time\n",
    "        \n",
    "    # Launch our kernel with n instances in our 1D grid.\n",
    "    n_rows = y.numel()\n",
    "    weighted_sum_fwd[(triton.cdiv(n_rows, ROWS_TILE_SIZE),)](\n",
    "            x, weight,\n",
    "            y,\n",
    "            x.stride(0), x.stride(1),\n",
    "            weight.stride(0),\n",
    "            y.stride(0),\n",
    "            ROWS=n_rows, D=D,\n",
    "            ROWS_TILE_SIZE=ROWS_TILE_SIZE, D_TILE_SIZE=D_TILE_SIZE,\n",
    "        )\n",
    "        \n",
    "    return y.view(output_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd16f2a-719a-4bfe-a254-5c40ee7bd4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_equal3(f1, f2):\n",
    "    x = torch.randn(64, 64, 2048, device=get_device())\n",
    "    w = torch.randn(2048, device=get_device())\n",
    "    y1 = f1(x,w)\n",
    "    y2 = f2(x,w)\n",
    "    assert torch.allclose(y1, y2, atol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c361f23-92f7-4543-90f7-07be6e6bdac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_equal3(weighted_sum,weighted_sum_triton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3769993-6ad7-45fe-8e4b-df074b8708a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedSumFunc(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, weight):\n",
    "        # Cache x and weight to be used in the backward pass, when we\n",
    "        # only receive the gradient wrt. the output tensor, and\n",
    "        # need to compute the gradients wrt. x and weight.\n",
    "        D, output_dims = x.shape[-1], x.shape[:-1]\n",
    "\n",
    "        # Reshape input tensor to 2D\n",
    "        x_reshaped = rearrange(x, \"... d -> (...) d\")\n",
    "        ctx.output_dims = output_dims\n",
    "\n",
    "        ctx.save_for_backward(x, weight)\n",
    "\n",
    "        assert len(weight.shape) == 1 and weight.shape[0] == D, \"Dimension mismatch\"\n",
    "        assert x.is_cuda and weight.is_cuda, \"Expected CUDA tensors\"\n",
    "        assert (\n",
    "            x_reshaped.is_contiguous()\n",
    "        ), \"Our pointer arithmetic will assume contiguous x\"\n",
    "\n",
    "        D_TILE_SIZE = (\n",
    "            triton.next_power_of_2(D) // 16\n",
    "        )  # Roughly 16 loops through the embedding dimension\n",
    "        ROWS_TILE_SIZE = 16  # Each thread processes 16 batch elements at a time\n",
    "\n",
    "        # Need to initialize empty result tensor. Note that these elements are not necessarily 0!\n",
    "        y = torch.empty(x_reshaped.shape[0], device=x.device)\n",
    "\n",
    "        # Launch our kernel with n instances in our 1D grid.\n",
    "        n_rows = y.numel()\n",
    "        weighted_sum_fwd[(triton.cdiv(n_rows, ROWS_TILE_SIZE),)](\n",
    "            x_reshaped,\n",
    "            weight,\n",
    "            y,\n",
    "            x_reshaped.stride(0),\n",
    "            x_reshaped.stride(1),\n",
    "            weight.stride(0),\n",
    "            y.stride(0),\n",
    "            ROWS=n_rows,\n",
    "            D=D,\n",
    "            ROWS_TILE_SIZE=ROWS_TILE_SIZE,\n",
    "            D_TILE_SIZE=D_TILE_SIZE,\n",
    "        )\n",
    "\n",
    "        return y.view(output_dims)\n",
    "\n",
    "    # Here you should make a triton kernel for the backward instead of plain PyTorch!\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Retrieve saved tensors\n",
    "        x, weight = ctx.saved_tensors\n",
    "\n",
    "        # Reshape grad_output to match forward pass\n",
    "        grad_output_flat = grad_output.reshape(-1)\n",
    "\n",
    "        # Gradient wrt weight: sum over all samples\n",
    "        # d/dw (w^T x) = x\n",
    "        # So grad_weight = sum_i grad_output[i] * x[i]\n",
    "        grad_weight = (grad_output_flat[:, None] * x).sum(dim=0)\n",
    "\n",
    "        # Gradient wrt x: broadcast weight\n",
    "        # d/dx (w^T x) = w\n",
    "        # So grad_x = grad_output * w\n",
    "        grad_x = grad_output_flat[:, None] * weight[None, :]\n",
    "\n",
    "        # Reshape grad_x back to original shape\n",
    "        grad_x = grad_x.view(*ctx.output_dims, -1)\n",
    "\n",
    "        return grad_x, grad_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0b14bf-2733-4732-9d7a-2c1795b20891",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionTriton(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Linear regression using the custom Triton weighted sum kernel.\n",
    "\n",
    "    Model: y = w^T x + b\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int):\n",
    "        super().__init__()\n",
    "        self.weight = torch.nn.Parameter(torch.randn(input_dim, device=\"cuda\") * 0.01)\n",
    "        self.bias = torch.nn.Parameter(torch.zeros(1, device=\"cuda\"))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, input_dim)\n",
    "        # Use our custom weighted sum function\n",
    "        return WeightedSumFunc.apply(x, self.weight) + self.bias\n",
    "\n",
    "\n",
    "def generate_regression_data(n_samples=1000, input_dim=128, noise_std=0.1, seed=42):\n",
    "    \"\"\"\n",
    "    Generate synthetic linear regression data.\n",
    "\n",
    "    Returns:\n",
    "        X: (n_samples, input_dim) feature matrix\n",
    "        y: (n_samples,) continuous target values\n",
    "        true_weight: (input_dim,) true weight vector used for generation\n",
    "        true_bias: (1,) true bias value used for generation\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Generate random features\n",
    "    X = torch.randn(n_samples, input_dim, device=\"cuda\")\n",
    "\n",
    "    # Create true weights for data generation\n",
    "    true_weight = torch.randn(input_dim, device=\"cuda\")\n",
    "    true_bias = torch.randn(1, device=\"cuda\")\n",
    "\n",
    "    # Generate target values: y = w^T x + b + noise\n",
    "    y = X @ true_weight + true_bias\n",
    "\n",
    "    # Add Gaussian noise\n",
    "    noise = torch.randn(n_samples, device=\"cuda\") * noise_std\n",
    "    y = y + noise\n",
    "\n",
    "    return X, y, true_weight, true_bias\n",
    "\n",
    "\n",
    "def train_linear_regression(\n",
    "    model, X_train, y_train, X_val, y_val, epochs=100, lr=0.01, batch_size=64\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the linear regression model.\n",
    "\n",
    "    Args:\n",
    "        model: LinearRegressionTriton model\n",
    "        X_train, y_train: Training data\n",
    "        X_val, y_val: Validation data\n",
    "        epochs: Number of training epochs\n",
    "        lr: Learning rate\n",
    "        batch_size: Batch size for training\n",
    "\n",
    "    Returns:\n",
    "        train_losses: List of training losses (MSE) per epoch\n",
    "        val_losses: List of validation losses (MSE) per epoch\n",
    "        val_r2_scores: List of validation R¬≤ scores per epoch\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_r2_scores = []\n",
    "\n",
    "    n_batches = (len(X_train) + batch_size - 1) // batch_size\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        # Shuffle data\n",
    "        perm = torch.randperm(len(X_train), device=\"cuda\")\n",
    "        X_train_shuffled = X_train[perm]\n",
    "        y_train_shuffled = y_train[perm]\n",
    "\n",
    "        for i in range(n_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = min(start_idx + batch_size, len(X_train))\n",
    "\n",
    "            X_batch = X_train_shuffled[start_idx:end_idx]\n",
    "            y_batch = y_train_shuffled[start_idx:end_idx]\n",
    "\n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / n_batches\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_val_pred = model(X_val)\n",
    "            val_loss = criterion(y_val_pred, y_val).item()\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "            # Calculate R¬≤ score\n",
    "            ss_res = ((y_val - y_val_pred) ** 2).sum()\n",
    "            ss_tot = ((y_val - y_val.mean()) ** 2).sum()\n",
    "            r2_score = 1 - (ss_res / ss_tot)\n",
    "            val_r2_scores.append(r2_score.item())\n",
    "\n",
    "        # Print progress every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch+1}/{epochs}: \"\n",
    "                f\"Train Loss = {avg_train_loss:.4f}, \"\n",
    "                f\"Val Loss = {val_loss:.4f}, \"\n",
    "                f\"Val R¬≤ = {r2_score.item():.4f}\"\n",
    "            )\n",
    "\n",
    "    return train_losses, val_losses, val_r2_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce9df3c-9b9d-48d2-a6b7-37ddd2d142ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to demonstrate linear regression with custom Triton kernel.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Linear Regression with Custom Triton Weighted Sum Kernel\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Check CUDA availability\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"ERROR: CUDA is not available. This example requires a GPU.\")\n",
    "        return\n",
    "\n",
    "    # Hyperparameters\n",
    "    n_train = 800\n",
    "    n_val = 200\n",
    "    input_dim = 128\n",
    "    epochs = 100\n",
    "    lr = 0.01\n",
    "    batch_size = 64\n",
    "    noise_std = 0.1\n",
    "\n",
    "    print(f\"\\nDataset configuration:\")\n",
    "    print(f\"  Training samples: {n_train}\")\n",
    "    print(f\"  Validation samples: {n_val}\")\n",
    "    print(f\"  Input dimension: {input_dim}\")\n",
    "    print(f\"  Noise std: {noise_std}\")\n",
    "    print(f\"\\nTraining configuration:\")\n",
    "    print(f\"  Epochs: {epochs}\")\n",
    "    print(f\"  Learning rate: {lr}\")\n",
    "    print(f\"  Batch size: {batch_size}\")\n",
    "    print()\n",
    "\n",
    "    # Generate data\n",
    "    print(\"Generating synthetic linear regression data...\")\n",
    "    X, y, true_weight, true_bias = generate_regression_data(\n",
    "        n_samples=n_train + n_val, input_dim=input_dim, noise_std=noise_std\n",
    "    )\n",
    "\n",
    "    # Split into train and validation\n",
    "    X_train, y_train = X[:n_train], y[:n_train]\n",
    "    X_val, y_val = X[n_train:], y[n_train:]\n",
    "\n",
    "    print(f\"Training set: X shape = {X_train.shape}, y shape = {y_train.shape}\")\n",
    "    print(f\"Validation set: X shape = {X_val.shape}, y shape = {y_val.shape}\")\n",
    "    print(f\"\\nTrue parameters:\")\n",
    "    print(f\"  True weight norm: {true_weight.norm().item():.6f}\")\n",
    "    print(f\"  True bias: {true_bias.item():.6f}\")\n",
    "    print()\n",
    "\n",
    "    # Initialize model\n",
    "    print(\"Initializing linear regression model with Triton kernel...\")\n",
    "    model = LinearRegressionTriton(input_dim=input_dim)\n",
    "    model = torch.compile(model)\n",
    "    print(\n",
    "        f\"Model parameters: weight shape = {model.weight.shape}, bias shape = {model.bias.shape}\"\n",
    "    )\n",
    "    print()\n",
    "\n",
    "    # Train model\n",
    "    print(\"Starting training...\\n\")\n",
    "    train_losses, val_losses, val_r2_scores = train_linear_regression(\n",
    "        model,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_val,\n",
    "        y_val,\n",
    "        epochs=epochs,\n",
    "        lr=lr,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    # Final evaluation\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Training Complete!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Final Training Loss (MSE): {train_losses[-1]:.4f}\")\n",
    "    print(f\"Final Validation Loss (MSE): {val_losses[-1]:.4f}\")\n",
    "    print()\n",
    "\n",
    "    # Compare learned parameters with true parameters\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Parameter Comparison: Learned vs True\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    learned_weight = model.weight.data\n",
    "    learned_bias = model.bias.data\n",
    "\n",
    "    # Compute various comparison metrics\n",
    "    weight_diff = learned_weight - true_weight\n",
    "    weight_mse = (weight_diff**2).mean().item()\n",
    "    weight_mae = weight_diff.abs().mean().item()\n",
    "    \n",
    "\n",
    "    bias_diff = (learned_bias - true_bias).abs().item()\n",
    "\n",
    "\n",
    "    print(f\"\\nWeight Statistics:\")\n",
    "    print(f\"  True weight norm:        {true_weight.norm().item():.6f}\")\n",
    "    print(f\"  Learned weight norm:     {learned_weight.norm().item():.6f}\")\n",
    "    print(f\"  Weight MSE:              {weight_mse:.6f}\")\n",
    "    print(f\"  Weight MAE:              {weight_mae:.6f}\")\n",
    "    \n",
    "    print(f\"\\nBias Statistics:\")\n",
    "    print(f\"  True bias:               {true_bias.item():.6f}\")\n",
    "    print(f\"  Learned bias:            {learned_bias.item():.6f}\")\n",
    "    print(f\"  Bias absolute difference: {bias_diff:.6f}\")\n",
    "    print()\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Example completed successfully!\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e789074-7bd9-4ec6-8c31-eacda7522441",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc87617-83f1-4839-880d-e5aba96500ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-syst",
   "language": "python",
   "name": "cs336-syst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}