{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cc70088-51e3-4989-b9d8-b938d01bb152",
   "metadata": {},
   "source": [
    "# Structured Generation\n",
    "\n",
    "## Motivation\n",
    "\n",
    "When we use an LLM to answer a question, the output is free-form text. But in many real-world applications we need the output in a **specific format**: a number, a JSON object, a date, a choice from a list, etc. The usual workaround is to generate free-form text and then parse it with a regular expression:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4efc233-6d47-4ea4-bd37-f2fcf2a53277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "answer = \"\"\"The first 10 digits of pi (π) are as follows:\n",
    "\n",
    "3.1415926535\n",
    "\"\"\"\n",
    "\n",
    "regex = r\"([0-9]+)?\\.[0-9]+\"\n",
    "print(re.search(regex, answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r0hvcbl6yt",
   "metadata": {},
   "source": [
    "This works, but it is fragile: the LLM might produce text that doesn't match the pattern at all, forcing us to retry or fail. **Structured generation** solves this by constraining the LLM _during_ generation so that every output is guaranteed to match the desired format.\n",
    "\n",
    "## Goal of this practical\n",
    "\n",
    "In this practical you will implement structured generation from scratch, progressing through three increasingly efficient approaches:\n",
    "\n",
    "1. **Naive approach** -- at each generation step, try every token in the vocabulary against a regex partial match. Simple but $O(V)$ per step.\n",
    "2. **DFA-based approach** -- compile the regex to a Deterministic Finite Automaton (DFA), then precompute which tokens are valid from each DFA state. The per-step cost drops to $O(1)$.\n",
    "3. **Coalescence** -- observe that many DFA states allow the exact same set of tokens, so we can precompute and _share_ mask arrays across equivalent states, eliminating even the array-allocation overhead.\n",
    "\n",
    "We follow the ideas of [Efficient Guided Generation for Large Language Models](https://arxiv.org/abs/2307.09702) by Brandon T. Willard and Remi Louf.\n",
    "\n",
    "## Setup\n",
    "\n",
    "Throughout the notebook we use a tiny toy vocabulary of 4 tokens: `[\"a\", \".\", \".2\", \"1\"]` and the regex pattern `([0-9]+)?\\.[0-9]+` (a decimal number like `3.14`). This keeps things small enough to inspect by hand. At the end, we scale up to GPT-2's real 50k-token vocabulary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a85e612-e870-4c6a-ae17-5c4644f36425",
   "metadata": {},
   "source": [
    "## Part 1: Naive constrained generation with regex partial matching\n",
    "\n",
    "The key idea is simple: at each generation step, we **mask out** every token that would make the output incompatible with the target regex. The LLM can then only sample from tokens that keep a valid match possible.\n",
    "\n",
    "Concretely, the algorithm works as follows:\n",
    "\n",
    "1. Start with an empty string as prefix.\n",
    "2. For every token in the vocabulary, concatenate it to the prefix and check whether the result is a **partial match** of the regex (i.e. could still lead to a full match if we kept appending characters).\n",
    "3. If not, set that token's logit to $-\\infty$ so it cannot be sampled.\n",
    "4. Sample the next token from the masked logits.\n",
    "5. Append the sampled token to the prefix and go back to step 2.\n",
    "\n",
    "The diagram below illustrates this process for the regex `[0-9]+\\.[0-9]` with the vocabulary `[\"a\", \".\", \".2\", \"1\"]`:\n",
    "\n",
    "![](https://cdn.prod.website-files.com/665725b00d910f65bec567fc/668c29d45780ee71a367c839_naive.png)\n",
    "\n",
    "### Partial matching\n",
    "\n",
    "A **partial match** is a string that matches the regex up to its last character -- one for which we could find a continuation that would produce a full match. Python's standard `re` library does not support partial matching, but the [regex](https://github.com/mrabarnett/mrab-regex) library does, via the `partial=True` flag:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c129a15e-2a6e-4dd0-92cf-16f4d9048f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "regex = r\"([0-9]+)?\\.[0-9]+\"\n",
    "print(re.fullmatch(regex, '.2', partial=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e51fa8-81c2-4e65-933a-bcfcaf5c6bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(re.fullmatch(regex, '1.2', partial=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41990eb-4625-4840-91fc-cf34096006a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(re.fullmatch(regex, '1.2a', partial=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juqlvzvqs1",
   "metadata": {},
   "source": [
    "### Exercise 1: Implement naive constrained generation\n",
    "\n",
    "Use `re.fullmatch(regex, string, partial=True)` to build a logit mask at each generation step. The mask should be a numpy array with `0` for tokens whose concatenation with the current prefix gives a partial match, and `-math.inf` otherwise. We simulate an LLM with uniform logits (all equal to 1) so the generation is purely driven by the mask.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f386a49-e925-4fa2-94c9-1f9ca62bda17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import regex as re\n",
    "import numpy as np\n",
    "\n",
    "from scipy.special import softmax\n",
    "\n",
    "np.random.seed(12349)\n",
    "\n",
    "logits = np.array([1., 1., 1., 1.])  # Random model with equal probabilities\n",
    "vocabulary = [\"a\", \".\", \".2\", \"1\"]\n",
    "\n",
    "regex = r\"([0-9]+)?\\.[0-9]+\"\n",
    "\n",
    "completion = \"\"\n",
    "for _ in range(7):\n",
    "\n",
    "    # Build the logit mask\n",
    "    # For each token in the vocabulary, check if appending it to the current\n",
    "    # completion could lead to a valid match (using partial matching).\n",
    "    # Set mask to 0 for valid tokens, -inf for invalid ones.\n",
    "    # The result should be a numpy array called `mask`.\n",
    "    #\n",
    "    # your code here\n",
    "    #\n",
    "    \n",
    "    masked_logits = logits + mask\n",
    "\n",
    "    # Sample the next token\n",
    "    probs = softmax(masked_logits)\n",
    "    next_token_id = np.random.choice(len(vocabulary), p=probs)\n",
    "\n",
    "    completion += vocabulary[next_token_id]\n",
    "\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e95486-edbe-4df1-ae51-89709b149fd9",
   "metadata": {},
   "source": [
    "## Part 2: DFA-based constrained generation\n",
    "\n",
    "The naive approach works correctly but is **far too slow** for real LLMs. With a vocabulary of $V \\approx 50{,}000$ tokens, we perform 50k regex partial matches _per generated token_. In Python this easily dominates inference time.\n",
    "\n",
    "The key insight is that regular expressions are equivalent to **Deterministic Finite Automata (DFA)**. A DFA is a directed graph where:\n",
    "- Each **node** is a state.\n",
    "- Each **edge** is a transition labeled with a character (or character class).\n",
    "- There is one **initial state** and one or more **accept (final) states**.\n",
    "\n",
    "To check whether a string matches, you walk the DFA one character at a time:\n",
    "\n",
    "1. Start in the initial state with the full string.\n",
    "2. Read the next character. If there is a matching transition, follow it to the next state. Otherwise, **reject**.\n",
    "3. After consuming the entire string, **accept** if you are in a final state.\n",
    "\n",
    "### Why does this help?\n",
    "\n",
    "Instead of running a regex engine on every `(prefix + token)` pair at every step, we can:\n",
    "1. **Precompute**, for each DFA state, which tokens lead to valid transitions.\n",
    "2. At generation time, just **look up** the current state to get the set of allowed tokens -- no regex matching needed.\n",
    "\n",
    "This trades an $O(V)$-per-step cost for a one-time $O(V \\times |\\text{states}|)$ precomputation.\n",
    "\n",
    "### Building the DFA\n",
    "\n",
    "We use the [interegular](https://github.com/MegaIng/interegular) library to convert our regex into its equivalent DFA. Let's see what it looks like:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b7e6d9-d678-4d0b-9eae-f7833733a08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import interegular\n",
    "\n",
    "regex = r\"([0-9]+)?\\.[0-9]+\"\n",
    "fsm = interegular.parse_pattern(regex).to_fsm()\n",
    "\n",
    "print(fsm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f5bef3-7538-4768-966d-e343da2e274c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fsm.alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f20be3-0b5c-4555-9f1b-6028e5bb7359",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (start, transitions) in fsm.map.items():\n",
    "    print(start, transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70635a5c-b249-40e3-b386-28592197615f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fsm.initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a737eea-c576-4031-94aa-38f183e8616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fsm.finals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8452c0-3ee4-4c4a-8bc3-b323f9552bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsm.alphabet.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x3xqgb8krlc",
   "metadata": {},
   "source": [
    "### Reading the DFA\n",
    "\n",
    "The DFA has three key components:\n",
    "\n",
    "- **`fsm.alphabet`** maps each character to a **symbol index**. Characters that behave identically (e.g. all digits `0`-`9`) share the same index. There is also an `anything_else` symbol for characters not explicitly listed.\n",
    "- **`fsm.map`** is the transition table: `fsm.map[state][symbol_index] = next_state`. If a `(state, symbol_index)` pair is missing, there is no valid transition (the string is rejected).\n",
    "- **`fsm.initial`** and **`fsm.finals`** are the start state and set of accept states.\n",
    "\n",
    "Let's visualize this DFA as a graph:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caba2f2-a3ed-486a-a71f-8c3168f6e0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from interegular import fsm as fsm_module\n",
    "from collections import defaultdict\n",
    "\n",
    "# Build edge labels dynamically from fsm.alphabet\n",
    "idx_to_chars = defaultdict(list)\n",
    "for char, idx in fsm.alphabet.items():\n",
    "    if char is fsm_module.anything_else:\n",
    "        idx_to_chars[idx].append(\"*\")\n",
    "    else:\n",
    "        idx_to_chars[idx].append(char)\n",
    "\n",
    "# Collapse groups into readable labels, e.g. ['0','1',...,'9'] -> \"[0-9]\"\n",
    "idx_to_label = {}\n",
    "for idx, chars in idx_to_chars.items():\n",
    "    if len(chars) > 3:\n",
    "        idx_to_label[idx] = f\"[{chars[0]}-{chars[-1]}]\"\n",
    "    else:\n",
    "        idx_to_label[idx] = \",\".join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fb3290-bd00-4a5f-bfca-6c2f59f201c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "from IPython.display import display\n",
    "\n",
    "# Define the regex pattern\n",
    "regex = r\"([0-9]+)?\\.[0-9]+\"\n",
    "\n",
    "# Convert regex to a finite state machine (FSM)\n",
    "fsm = interegular.parse_pattern(regex).to_fsm()\n",
    "\n",
    "# Generate Graphviz DOT format representation\n",
    "dot = graphviz.Digraph(format=\"png\")\n",
    "\n",
    "# Add states to the graph\n",
    "for state in fsm.states:\n",
    "    shape = \"doublecircle\" if state in fsm.finals else \"circle\"\n",
    "    dot.node(str(state), shape=shape)\n",
    "\n",
    "# Add transitions to the graph\n",
    "for (start, transitions) in fsm.map.items():\n",
    "    for char, end in transitions.items():\n",
    "        dot.edge(str(start), str(end), label=idx_to_label.get(char, str(char)))\n",
    "\n",
    "display(dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4351b6b-c71c-4ab7-b446-e94a59c3d539",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsm.map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1c828d-3dd9-4375-9eb4-3364b00bc613",
   "metadata": {},
   "source": [
    "### From characters to tokens: walking the DFA\n",
    "\n",
    "The DFA operates on **characters**, but LLMs generate **tokens** (which can be multi-character strings like `\".2\"` or `\"1\"`). To check whether a token is compatible with a given DFA state, we need to \"walk\" the token through the DFA one character at a time.\n",
    "\n",
    "Given a starting state and a token string, we:\n",
    "1. Look up the first character in `fsm.alphabet` to get its symbol index.\n",
    "2. Check if there is a transition from the current state for that symbol. If not, the token is **rejected** from this state.\n",
    "3. Follow the transition to the next state and repeat for the remaining characters.\n",
    "4. If we consume all characters without rejection, the token is **valid** from this state.\n",
    "\n",
    "For example, starting from state 0 with token `\".2\"`:\n",
    "- Character `\".\"` has symbol index 2, and `fsm.map[0][2] = 2` $\\Rightarrow$ move to state 2.\n",
    "- Character `\"2\"` has symbol index 0 (a digit), and `fsm.map[2][0] = 4` $\\Rightarrow$ move to state 4.\n",
    "- We traversed states $(0, 2, 4)$. State 4 is a final state, so `\".2\"` is a complete valid match!\n",
    "\n",
    "Conversely, token `\"a\"` from state 0: character `\"a\"` maps to `anything_else` (symbol index 1), and there is no transition `fsm.map[0][1]`, so `\"a\"` is rejected.\n",
    "\n",
    "### Exercise 2: Implement `partial_match`\n",
    "\n",
    "Write a function that walks a token through the DFA and returns the tuple of traversed states, or `None` if the token is rejected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fb53ac-1f82-4144-b9fa-01630e1b05f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_match(state, token):\n",
    "    \"\"\"Partially match the token to the DFA starting from `state`.\n",
    "\n",
    "    We iterate over the token's symbols, and at each step transition to the \n",
    "    next state if we find a valid transition. \n",
    "    If there is a stage without a valid transision, we return None, otherwise\n",
    "    we return a tuple that contains the sequence of traversed states.\n",
    "\n",
    "    Hints:\n",
    "    - Use fsm.alphabet[symbol] to get the alphabet index of a character.\n",
    "    - Use fsm.map[state] to get the transitions from a state.\n",
    "    - Return a tuple of all traversed states (including the starting state).\n",
    "    \"\"\"\n",
    "    \n",
    "    traversed_states = (state,)\n",
    "    # Iterate over the token's symbols, trying at each step to transition\n",
    "    # to a new DFA state.\n",
    "    #\n",
    "    # your code here\n",
    "    #\n",
    "    \n",
    "    return traversed_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fccc57-7790-4e58-83ee-58d228711334",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \".21\"\n",
    "print(partial_match(0, token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89109d5f-478a-4163-b017-cfa2938a2541",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \".21.\"\n",
    "print(partial_match(0, token))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c500642f-a8b9-473f-b475-11eba46bb11e",
   "metadata": {},
   "source": [
    "### Exercise 3: Build the state-to-token index\n",
    "\n",
    "Now we need to precompute, for every DFA state, which tokens from our vocabulary correspond to a valid transition. We build two data structures:\n",
    "\n",
    "- **`states_to_vocab[state]`**: the set of token IDs that are valid from `state`.\n",
    "- **`states_token_states[state][token_id]`**: the DFA state we land on after consuming that token from `state`.\n",
    "\n",
    "This is the one-time precomputation that makes generation fast: instead of running the regex for every token at every step, we just look up `states_to_vocab[current_state]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2de703-3602-4ea2-8f18-37755c61e1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "vocabulary = [\"a\", \".\", \".2\", \"1\"]\n",
    "\n",
    "# Map from the DFA states to the tokens that correspond to a valid transition\n",
    "# from this state.\n",
    "states_to_vocab = defaultdict(set)\n",
    "states_token_states = defaultdict(dict)\n",
    "\n",
    "# Iterate (once) through the vocabulary and for each token, check from each\n",
    "# DFA state whether partial_match finds a valid path.\n",
    "# If so, record the token_id in states_to_vocab[state] and the landing state\n",
    "# in states_token_states[state][token_id].\n",
    "#\n",
    "# your code here\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1372f2-65eb-45a5-a316-e00e0d351caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "states_to_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ca3bca-6622-430d-88b2-4cf88e1ef738",
   "metadata": {},
   "source": [
    "### Exercise 4: DFA-based generation\n",
    "\n",
    "With the index built, generation becomes straightforward:\n",
    "\n",
    "1. Start in the initial DFA state. Look up `states_to_vocab[state]` to get the allowed tokens.\n",
    "2. Build a mask: $-\\infty$ everywhere, then set the allowed positions to 0.\n",
    "3. Add the mask to the logits and sample.\n",
    "4. Look up `states_token_states[state][token_id]` to transition to the next DFA state.\n",
    "5. Repeat.\n",
    "\n",
    "You should get the **same result** as the naive approach (`11.21111`) since the random seed is the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3614e2ee-d54a-497a-bd5d-ca9fc5211472",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12349) # you should get the same result as before\n",
    "\n",
    "logits = np.array([1., 1., 1., 1.])  # same as before\n",
    "\n",
    "regex = r\"([0-9]+)?\\.[0-9]+\"\n",
    "\n",
    "completion = \"\"\n",
    "state = fsm.initial\n",
    "for _ in range(7):\n",
    "\n",
    "    # Build the logit mask using states_to_vocab[state]\n",
    "    # (no regex needed — just a set lookup!)\n",
    "    #\n",
    "    # your code here\n",
    "    #\n",
    "\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0473bd9",
   "metadata": {},
   "source": [
    "### A small Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bae294",
   "metadata": {},
   "source": [
    "We now benchmark the two approaches (naive regex partial matching vs DFA-based constrained decoding) across increasing vocabulary sizes. This demonstrates why the naive approach becomes impractical as the vocabulary grows to realistic LLM sizes (~50k tokens), and why precomputing valid transitions via a DFA is essential.\n",
    "\n",
    "We build a synthetic vocabulary with a small \"useful\" core of digit/dot tokens and pad the rest with junk tokens that will never match (a realistic scenario where most tokens are irrelevant to the regex)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w5573pnpy9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "REGEX = r\"([0-9]+)?\\.[0-9]+\"\n",
    "\n",
    "def build_vocabulary(size: int):\n",
    "    \"\"\"Build a synthetic vocabulary: a small useful core + junk padding tokens.\"\"\"\n",
    "    core = [\n",
    "        \".\", \"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\",\n",
    "        \".0\", \".1\", \".2\", \".3\", \".4\", \".5\", \".6\", \".7\", \".8\", \".9\",\n",
    "        \"10\", \"12\", \"42\", \"100\", \"256\",\n",
    "    ]\n",
    "    padding = [f\"tok_{i}\" for i in range(size - len(core))]\n",
    "    vocab = core + padding\n",
    "    return vocab[:size]\n",
    "\n",
    "\n",
    "def naive_mask(completion, vocabulary, pattern):\n",
    "    \"\"\"Approach 1: one regex partial match per token per step — O(V) per token.\"\"\"\n",
    "    mask = []\n",
    "    for token in vocabulary:\n",
    "        tentative = completion + token\n",
    "        if re.fullmatch(pattern, tentative, partial=True) is None:\n",
    "            mask.append(-math.inf)\n",
    "        else:\n",
    "            mask.append(0.0)\n",
    "    return np.array(mask)\n",
    "\n",
    "\n",
    "def build_dfa_index(vocabulary, pattern):\n",
    "    \"\"\"Approach 2: one-time precomputation — build the DFA and index valid tokens per state.\"\"\"\n",
    "    fsm = interegular.parse_pattern(pattern).to_fsm()\n",
    "\n",
    "    def _partial_match(state, token):\n",
    "        traversed = (state,)\n",
    "        for symbol in token:\n",
    "            alphabet_idx = fsm.alphabet.get(symbol)\n",
    "            if alphabet_idx is None:\n",
    "                alphabet_idx = fsm.alphabet.get(interegular.fsm.anything_else)\n",
    "            if state not in fsm.map or alphabet_idx not in fsm.map[state]:\n",
    "                return None\n",
    "            state = fsm.map[state][alphabet_idx]\n",
    "            traversed += (state,)\n",
    "        return traversed\n",
    "\n",
    "    states_to_vocab = defaultdict(set)\n",
    "    states_token_states = defaultdict(dict)\n",
    "\n",
    "    for token_id, token in enumerate(vocabulary):\n",
    "        for state in fsm.map:\n",
    "            path = _partial_match(state, token)\n",
    "            if path is not None:\n",
    "                states_to_vocab[state].add(token_id)\n",
    "                states_token_states[state][token_id] = path[-1]\n",
    "\n",
    "    return fsm, states_to_vocab, states_token_states\n",
    "\n",
    "\n",
    "def dfa_mask(state, states_to_vocab, vocab_size):\n",
    "    \"\"\"O(1) lookup per token using the precomputed index.\"\"\"\n",
    "    mask = np.full(vocab_size, -np.inf)\n",
    "    valid = list(states_to_vocab[state])\n",
    "    if valid:\n",
    "        mask[valid] = 0.0\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0lmva63znql",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(vocab_sizes, n_steps=7, n_repeats=3):\n",
    "    results = []\n",
    "\n",
    "    print(f\"Regex: {REGEX}\")\n",
    "    print(f\"Generating {n_steps} tokens per run, median of {n_repeats} repeats\\n\")\n",
    "    print(f\"{'Vocab size':>12}  {'Naive (ms/step)':>16}  {'DFA mask (ms/step)':>18}  {'DFA precomp (ms)':>16}  {'Speedup':>8}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for V in vocab_sizes:\n",
    "        vocabulary = build_vocabulary(V)\n",
    "        logits = np.ones(V)\n",
    "\n",
    "        # ---- Naive timing ----\n",
    "        naive_times = []\n",
    "        for _ in range(n_repeats):\n",
    "            np.random.seed(42)\n",
    "            completion = \"\"\n",
    "            t0 = time.perf_counter()\n",
    "            for _ in range(n_steps):\n",
    "                mask = naive_mask(completion, vocabulary, REGEX)\n",
    "                masked_logits = logits + mask\n",
    "                probs = softmax(masked_logits)\n",
    "                next_id = np.random.choice(V, p=probs)\n",
    "                completion += vocabulary[next_id]\n",
    "            naive_times.append((time.perf_counter() - t0) / n_steps)\n",
    "        naive_ms = np.median(naive_times) * 1000\n",
    "\n",
    "        # ---- DFA precomputation ----\n",
    "        t0 = time.perf_counter()\n",
    "        fsm_bench, s2v, sts = build_dfa_index(vocabulary, REGEX)\n",
    "        precomp_ms = (time.perf_counter() - t0) * 1000\n",
    "\n",
    "        # ---- DFA mask timing ----\n",
    "        dfa_times = []\n",
    "        for _ in range(n_repeats):\n",
    "            np.random.seed(42)\n",
    "            state = fsm_bench.initial\n",
    "            completion = \"\"\n",
    "            t0 = time.perf_counter()\n",
    "            for _ in range(n_steps):\n",
    "                mask = dfa_mask(state, s2v, V)\n",
    "                masked_logits = logits + mask\n",
    "                probs = softmax(masked_logits)\n",
    "                next_id = np.random.choice(V, p=probs)\n",
    "                state = sts[state][next_id]\n",
    "                completion += vocabulary[next_id]\n",
    "            dfa_times.append((time.perf_counter() - t0) / n_steps)\n",
    "        dfa_ms = np.median(dfa_times) * 1000\n",
    "\n",
    "        speedup = naive_ms / dfa_ms if dfa_ms > 0 else float(\"inf\")\n",
    "        results.append((V, naive_ms, dfa_ms, precomp_ms, speedup))\n",
    "        print(f\"{V:>12,}  {naive_ms:>14.2f}ms  {dfa_ms:>16.3f}ms  {precomp_ms:>14.1f}ms  {speedup:>7.0f}x\")\n",
    "\n",
    "    return results\n",
    "\n",
    "results = benchmark([100, 500, 1_000, 5_000, 10_000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psdjyeq3in",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "vocab_sizes = [r[0] for r in results]\n",
    "naive_times = [r[1] for r in results]\n",
    "dfa_times = [r[2] for r in results]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.plot(vocab_sizes, naive_times, \"o-\", label=\"Naive (regex partial match)\")\n",
    "ax.plot(vocab_sizes, dfa_times, \"s-\", label=\"DFA (precomputed index)\")\n",
    "ax.set_xlabel(\"Vocabulary size\")\n",
    "ax.set_ylabel(\"Time per generation step (ms)\")\n",
    "ax.set_title(\"Constrained Decoding: Naive vs DFA\")\n",
    "ax.legend()\n",
    "ax.set_yscale(\"log\")\n",
    "ax.grid(True, which=\"both\", ls=\"--\", alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67opgh1hwze",
   "metadata": {},
   "source": [
    "**Key takeaway:**\n",
    "- **Naive:** O(V) partial matches **per generated token** — linear in vocab size, dominates generation time.\n",
    "- **DFA:** O(V x |states|) **one-time** precomputation, then O(1) lookup per token.\n",
    "- At V=10k+, the naive approach is orders of magnitude slower. At realistic LLM vocab sizes (~50k), it becomes completely impractical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xf76xshhaog",
   "metadata": {},
   "source": [
    "## Coalescence: precomputing masks\n",
    "\n",
    "The DFA approach already gives a huge speedup: instead of running a regex partial match per token, we do a set lookup. But there is still overhead at each generation step: we allocate a numpy array (`np.full(V, -inf)`) and set the valid indices to 0.\n",
    "\n",
    "**Coalescence** observes that many DFA states share the *exact same* set of allowed tokens. For our regex, look at `states_to_vocab` above: states 4 and 5 both allow `{3}`, states 0, 1, and 3 all allow `{1, 2, 3}`. Why recompute the same mask for equivalent states?\n",
    "\n",
    "The idea:\n",
    "1. Build the token-level FSM (using `token_fsm.py`)\n",
    "2. Group states by their `frozenset` of allowed token IDs\n",
    "3. **Precompute one mask array per group** during the one-time preprocessing\n",
    "4. Build a lookup: `state → precomputed_mask`\n",
    "5. At generation time: `mask = precomputed_masks[state]` — a single dict lookup, **zero array allocation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sgo736awl2i",
   "metadata": {},
   "outputs": [],
   "source": [
    "from token_fsm import make_deterministic_fsm, create_fsm_index_tokenizer\n",
    "\n",
    "\n",
    "def build_coalesced_index(vocabulary, pattern):\n",
    "    \"\"\"Build the token-level FSM and precompute one mask per unique allowed-token set.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    token_fsm : TokenFSM with .map[state][token_id] -> next_state\n",
    "    precomputed_masks : dict[state] -> np.array (the logit mask, ready to use)\n",
    "    precomp_ms : total precomputation time in milliseconds\n",
    "    \"\"\"\n",
    "    V = len(vocabulary)\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    # Step 1: regex -> character-level DFA -> clean up\n",
    "    raw_fsm = interegular.parse_pattern(pattern).to_fsm()\n",
    "    clean_fsm, _ = make_deterministic_fsm(raw_fsm)\n",
    "\n",
    "    # Step 2: build token-level FSM\n",
    "    tok_fsm, index = create_fsm_index_tokenizer(clean_fsm, vocabulary)\n",
    "\n",
    "    # Step 3: coalescence — group states by their allowed token set\n",
    "    # For each state in clean_fsm.states, get the frozenset of allowed token IDs\n",
    "    # from tok_fsm.map. States with the same allowed set should share the same\n",
    "    # precomputed mask (a numpy array of shape (V,) with 0 for allowed tokens\n",
    "    # and -inf for the rest).\n",
    "    # Build:\n",
    "    #   mask_cache: frozenset -> np.array (one mask per unique token set)\n",
    "    #   precomputed_masks: state -> np.array (lookup for generation)\n",
    "    #\n",
    "    # your code here\n",
    "    #\n",
    "\n",
    "    precomp_ms = (time.perf_counter() - t0) * 1000\n",
    "\n",
    "    n_states = len(clean_fsm.states)\n",
    "    n_groups = len(mask_cache)\n",
    "    print(f\"  Coalescence: {n_states} states -> {n_groups} unique masks\")\n",
    "\n",
    "    return tok_fsm, precomputed_masks, precomp_ms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yu417o5c2gl",
   "metadata": {},
   "source": [
    "### Exercise 5: Implement coalescence\n",
    "\n",
    "The helper module `token_fsm.py` provides two functions:\n",
    "- **`make_deterministic_fsm(fsm)`** cleans up the character-level DFA (remaps states to contiguous integers).\n",
    "- **`create_fsm_index_tokenizer(fsm, vocabulary)`** builds a **token-level FSM** -- an FSM whose transitions are over token IDs instead of characters. It returns a `TokenFSM` object with a `.map[state][token_id] = next_state` transition table.\n",
    "\n",
    "Your task is to implement the coalescence step: group states that share the same set of allowed tokens, precompute one mask per group, and build a `precomputed_masks` dict.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27792028-54ee-4127-aee9-2d19dfa9ac75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "regex_pattern = r\"([0-9]+)?\\.[0-9]+\"\n",
    "vocabulary = [\"a\", \".\", \".2\", \"1\"]\n",
    "\n",
    "print(f\"Regex:      {regex_pattern}\")\n",
    "print(f\"Vocabulary: {vocabulary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40z5t7ma9m4",
   "metadata": {},
   "source": [
    "### Walking through the token-level FSM step by step\n",
    "\n",
    "Before using `build_coalesced_index`, let's see how the pipeline from `token_fsm.py` works on our toy example. We go through each step: parsing the regex, cleaning the DFA, building the token-level FSM, and running constrained generation with it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rb87urjnmtl",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Parse regex to character-level DFA ---\n",
    "raw_fsm = interegular.parse_pattern(regex_pattern).to_fsm()\n",
    "print(\"── Raw character-level DFA ──\")\n",
    "print(f\"  States:  {raw_fsm.states}\")\n",
    "print(f\"  Initial: {raw_fsm.initial}\")\n",
    "print(f\"  Finals:  {raw_fsm.finals}\")\n",
    "print(f\"  Transitions:\")\n",
    "for state, trans in sorted(raw_fsm.map.items(), key=lambda x: str(x[0])):\n",
    "    print(f\"    State {state}: {dict(trans)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "axngyp7js2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: Clean up the DFA ---\n",
    "clean_fsm, state_mapping = make_deterministic_fsm(raw_fsm)\n",
    "print(f\"── Cleaned DFA (state mapping: {state_mapping}) ──\")\n",
    "print(f\"  States:  {clean_fsm.states}\")\n",
    "print(f\"  Initial: {clean_fsm.initial}\")\n",
    "print(f\"  Finals:  {clean_fsm.finals}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549wgavmi3r",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: Build token-level FSM ---\n",
    "token_fsm, index = create_fsm_index_tokenizer(clean_fsm, vocabulary)\n",
    "\n",
    "print(f\"── Token-level FSM ──\")\n",
    "print(f\"  Initial state: {token_fsm.initial}\")\n",
    "print(f\"  Accept states: {token_fsm.finals}\")\n",
    "print(f\"\\n  Transition table (state → token → next_state):\")\n",
    "for state in sorted(token_fsm.map.keys()):\n",
    "    for tid, next_s in sorted(token_fsm.map[state].items()):\n",
    "        print(f\"    State {state} --[{tid}: '{vocabulary[tid]}']→ State {next_s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ixe0ia3kpbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: Constrained generation using the token FSM ---\n",
    "print(\"── Constrained generation ──\")\n",
    "\n",
    "np.random.seed(12349)\n",
    "logits = np.ones(len(vocabulary))\n",
    "\n",
    "completion = \"\"\n",
    "state = token_fsm.initial\n",
    "\n",
    "for step in range(7):\n",
    "    # Masking is now just a set lookup — no regex needed!\n",
    "    allowed = token_fsm.allowed_token_ids(state)\n",
    "    mask = np.full(len(vocabulary), -np.inf)\n",
    "    mask[list(allowed)] = 0.0\n",
    "\n",
    "    masked_logits = logits + mask\n",
    "    probs = softmax(masked_logits)\n",
    "    next_id = np.random.choice(len(vocabulary), p=probs)\n",
    "\n",
    "    next_state = token_fsm.next_state(state, next_id)\n",
    "    print(f\"  Step {step}: state={state}, \"\n",
    "          f\"allowed={[vocabulary[i] for i in sorted(allowed)]}, \"\n",
    "          f\"sampled='{vocabulary[next_id]}' → state={next_state}\")\n",
    "\n",
    "    state = next_state\n",
    "    completion += vocabulary[next_id]\n",
    "\n",
    "print(f\"\\n  Final completion: '{completion}'\")\n",
    "is_full_match = state in token_fsm.finals\n",
    "print(f\"  In accept state?  {is_full_match}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0qrfgsigf28q",
   "metadata": {},
   "source": [
    "## Part 4: Applying to a JSON schema with a real tokenizer\n",
    "\n",
    "So far we used a toy vocabulary of 4 tokens. In practice, LLMs use **BPE (Byte-Pair Encoding) tokenizers** with ~50k tokens, where a single token can be a multi-character subword like `\"name\"`, `\":\"`, or `\"John\"`.\n",
    "\n",
    "This creates an important subtlety: a single BPE token can span **multiple DFA transitions** at once. For example, the token `\"name\"` walks through 4 character-level DFA states in one step. This is exactly why we need a **token-level FSM** rather than a character-level one.\n",
    "\n",
    "Let's see how the token-level FSM handles a JSON-structured regex with GPT-2's real tokenizer. The pattern enforces a specific JSON schema:\n",
    "\n",
    "```\n",
    "\\{\"name\":(\"John\"|\"Paul\"),\"age\":(20|30)\\}\n",
    "```\n",
    "\n",
    "This is the bridge between **structured output** (JSON) and **constrained decoding**: we express the schema as a regex, compile it to a DFA, then build the token-level index over the real BPE vocabulary.\n",
    "\n",
    "### Exercise 6: Implement `walk_token_fsm` and build the index\n",
    "\n",
    "This exercise is similar to Exercise 2 (`partial_match`), but now you must also handle the `anything_else` alphabet symbol (for characters not explicitly in the regex), and build the full index over GPT-2's 50k vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pgqzvyy94jk",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_pattern = r'\\{\"name\":(\"John\"|\"Paul\"),\"age\":(20|30)\\}'\n",
    "\n",
    "# Build character-level DFA\n",
    "raw_fsm = interegular.parse_pattern(json_pattern).to_fsm()\n",
    "json_fsm, _ = make_deterministic_fsm(raw_fsm)\n",
    "\n",
    "print(f\"Pattern: {json_pattern}\")\n",
    "print(f\"DFA: {len(json_fsm.states)} states, {len(json_fsm.finals)} accept state(s)\")\n",
    "print(f\"\\nCharacter-level DFA transitions:\")\n",
    "for state in sorted(json_fsm.map.keys()):\n",
    "    print(f\"  State {state}: {dict(json_fsm.map[state])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dyd52jla",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from interegular import fsm as fsm_module\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "def walk_token_fsm(fsm, state, token_str):\n",
    "    \"\"\"Walk a token string through the character-level DFA.\n",
    "    \n",
    "    For each character in token_str, look up its alphabet symbol index,\n",
    "    then check if there's a valid transition from the current state.\n",
    "    Return the final state if the full token is consumed, or None if\n",
    "    any character has no valid transition.\n",
    "\n",
    "    Hints:\n",
    "    - Use fsm.alphabet to map characters to symbol indices.\n",
    "    - Handle characters not in the alphabet using fsm_module.anything_else.\n",
    "    - Use fsm.map.get(state, {}) for transitions.\n",
    "    \"\"\"\n",
    "    #\n",
    "    # your code here\n",
    "    #\n",
    "\n",
    "print(f\"Tokenizer vocab size: {tokenizer.vocab_size:,}\")\n",
    "print(\"Building token-level FSM index (this may take a minute)...\")\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "json_index = defaultdict(dict)\n",
    "# For each DFA state and each token in the vocabulary, check if the token\n",
    "# can walk through the DFA starting from that state using walk_token_fsm.\n",
    "# If so, record the landing state in json_index[state][token_id].\n",
    "# Use tokenizer.decode([token_id]) to get the token string.\n",
    "#\n",
    "# your code here\n",
    "#\n",
    "elapsed = time.perf_counter() - t0\n",
    "\n",
    "print(f\"Index built in {elapsed:.1f}s\")\n",
    "print(f\"States with valid transitions: {len(json_index)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ieabbhk7c0r",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display: for each state, show the decoded BPE tokens and their target states\n",
    "for state in sorted(json_index.keys()):\n",
    "    transitions_decoded = {\n",
    "        repr(tokenizer.decode([tid])): next_s\n",
    "        for tid, next_s in json_index[state].items()\n",
    "    }\n",
    "    print(f\"State {state}: {transitions_decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "septlvitmn9",
   "metadata": {},
   "source": [
    "Notice how BPE multi-character tokens like `'name'`, `'\":\"'`, `'John'`, `',\"'` each correspond to valid multi-step transitions through the DFA. At each generation step, the LLM can jump several DFA states at once by emitting a single multi-character token. This is exactly why we need a **token-level** FSM rather than a character-level one: real tokenizers don't emit one character at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9x3bm9886dh",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_char_dfa(fsm):\n",
    "    \"\"\"Plot the character-level DFA with graphviz.\"\"\"\n",
    "    idx_to_chars = defaultdict(list)\n",
    "    for char, idx in fsm.alphabet.items():\n",
    "        if char is fsm_module.anything_else:\n",
    "            idx_to_chars[idx].append(\"*\")\n",
    "        elif char == \" \":\n",
    "            idx_to_chars[idx].append(\"⎵\")\n",
    "        else:\n",
    "            idx_to_chars[idx].append(char)\n",
    "\n",
    "    dot = graphviz.Digraph(\n",
    "        name=\"Character-level DFA\",\n",
    "        graph_attr={\"rankdir\": \"LR\", \"dpi\": \"50\", \"fontsize\": \"12\"},\n",
    "        node_attr={\"fontsize\": \"11\"},\n",
    "        edge_attr={\"fontsize\": \"9\"},\n",
    "    )\n",
    "    dot.node(\"start\", shape=\"point\", width=\"0\")\n",
    "    dot.edge(\"start\", str(fsm.initial))\n",
    "\n",
    "    for state in sorted(fsm.states):\n",
    "        shape = \"doublecircle\" if state in fsm.finals else \"circle\"\n",
    "        dot.node(str(state), str(state), shape=shape)\n",
    "\n",
    "    edge_labels = defaultdict(list)\n",
    "    for state, transitions in fsm.map.items():\n",
    "        for sym_idx, target in transitions.items():\n",
    "            chars = idx_to_chars.get(sym_idx, [f\"[{sym_idx}]\"])\n",
    "            edge_labels[(str(state), str(target))].append(\",\".join(chars))\n",
    "\n",
    "    for (src, dst), labels in edge_labels.items():\n",
    "        dot.edge(src, dst, label=\" | \".join(labels))\n",
    "    return dot\n",
    "\n",
    "\n",
    "def plot_token_fsm(index, vocabulary, initial, finals):\n",
    "    \"\"\"Plot the token-level FSM with BPE tokens as edge labels.\"\"\"\n",
    "    dot = graphviz.Digraph(\n",
    "        name=\"Token-level FSM\",\n",
    "        graph_attr={\"rankdir\": \"LR\", \"dpi\": \"45\", \"fontsize\": \"12\"},\n",
    "        node_attr={\"fontsize\": \"11\"},\n",
    "        edge_attr={\"fontsize\": \"9\"},\n",
    "    )\n",
    "    all_states = {initial} | set(finals)\n",
    "    for state, transitions in index.items():\n",
    "        all_states.add(state)\n",
    "        for tid, target in transitions.items():\n",
    "            all_states.add(target)\n",
    "\n",
    "    dot.node(\"start\", shape=\"point\", width=\"0\")\n",
    "    dot.edge(\"start\", str(initial))\n",
    "\n",
    "    for state in sorted(all_states):\n",
    "        shape = \"doublecircle\" if state in finals else \"circle\"\n",
    "        dot.node(str(state), str(state), shape=shape)\n",
    "\n",
    "    edge_labels = defaultdict(list)\n",
    "    for state, transitions in index.items():\n",
    "        for tid, target in transitions.items():\n",
    "            token_str = vocabulary[tid]\n",
    "            disp = token_str.replace('\"', '\\\\\"').replace(\"{\", \"\\\\{\").replace(\"}\", \"\\\\}\")\n",
    "            edge_labels[(str(state), str(target))].append(f'\"{disp}\"')\n",
    "\n",
    "    for (src, dst), labels in edge_labels.items():\n",
    "        dot.edge(src, dst, label=\" | \".join(labels))\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dsalevspma",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(plot_char_dfa(json_fsm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4qegh8t0zim",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = [tokenizer.decode([i]) for i in range(tokenizer.vocab_size)]\n",
    "display(plot_token_fsm(json_index, vocab_list, json_fsm.initial, json_fsm.finals))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dldiy",
   "language": "python",
   "name": "dldiy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
