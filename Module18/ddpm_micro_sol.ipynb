{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5303405d",
   "metadata": {},
   "source": [
    "[![Dataflowr](https://raw.githubusercontent.com/dataflowr/website/master/_assets/dataflowr_logo.png)](https://dataflowr.github.io/website/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c28fae",
   "metadata": {},
   "source": [
    "# [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239) for CIFAR10\n",
    "(J. Ho, A. Jain, P. Abbeel 2020)\n",
    "\n",
    "![](https://raw.githubusercontent.com/dataflowr/website/master/modules/extras/diffusions/ddpm.png)\n",
    "\n",
    "\n",
    "Given a schedule $\\beta_1<\\beta_2<\\dots <\\beta_T$, the **forward diffusion process** is defined by:\n",
    "$q(x_t|x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1-\\beta_t}x_{t-1},\\beta_t I)$ and $q(x_{1:T}|x_0) = \\prod_{t=1}^T q(x_t|x_{t-1})$.\n",
    "\n",
    "With $\\alpha_t = 1-\\beta_t$ and $\\overline{\\alpha_t} = \\prod_{i=1}^t\\alpha_i$, we see that, with $\\epsilon\\sim\\mathcal{N}(0,I)$:\n",
    "\\begin{align*}\n",
    "x_t = \\sqrt{\\overline{\\alpha}_t}x_0 + \\sqrt{1-\\overline{\\alpha}_t}\\epsilon.\n",
    "\\end{align*}\n",
    "The law $q(x_{t-1}|x_t,\\epsilon)$ is explicit: $q(x_{t-1}|x_t,\\epsilon) = \\mathcal{N}(x_{t-1};\\mu(x_t,\\epsilon,t), \\gamma_t I)$ with,\n",
    "\\begin{align*}\n",
    "\\mu(x_t,\\epsilon, t) = \\frac{1}{\\sqrt{\\alpha_t}}\\left( x_t-\\frac{1-\\alpha_t}{\\sqrt{1-\\overline{\\alpha}_t}}\\epsilon\\right)\\text{ and, }\n",
    "\\gamma_t = \\frac{1-\\overline{\\alpha}_{t-1}}{1-\\overline{\\alpha}_{t}}\\beta_t\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "**Training**: to approximate **the reversed diffusion** $q(x_{t-1}|x_t)$ by a neural network given by $p_{\\theta}(x_{t-1}|x_t) = \\mathcal{N}(x_{t-1}; \\mu_{\\theta}(x_t,t), \\beta_t I)$ and $p(x_T) \\sim \\mathcal{N}(0,I)$, we maximize the usual Variational bound:\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{q(x_0)} \\ln p_{\\theta}(x_0) &\\geq L_T +\\sum_{t=2}^T L_{t-1}+L_0 \\text{ with, }L_{t-1} = \\mathbb{E}_q\\left[ \\frac{1}{2\\sigma_t^2}\\|\\mu_\\theta(x_t,t) -\\mu(x_t,\\epsilon,t)\\|^2\\right].\n",
    "\\end{align*}\n",
    "With the change of variable:\n",
    "\\begin{align*}\n",
    "\\mu_\\theta(x_t,t) = \\frac{1}{\\sqrt{\\alpha_t}}\\left( x_t-\\frac{1-\\alpha_t}{\\sqrt{1-\\overline{\\alpha}_t}}\\epsilon_\\theta(x_t,t)\\right),\n",
    "\\end{align*}\n",
    "ignoring the prefactor and sampling $\\tau$ instead of summing over all $t$, the loss is finally:\n",
    "\\begin{align*}\n",
    "\\ell(\\theta) = \\mathbb{E}_\\tau\\mathbb{E}_\\epsilon \\left[ \\|\\epsilon - \\epsilon_\\theta(\\sqrt{\\overline{\\alpha}_\\tau}x_0 + \\sqrt{1-\\overline{\\alpha}_\\tau}\\epsilon, \\tau)\\|^2\\right]\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\n",
    "**Sampling**: to simulate the reversed diffusion with the learned $\\epsilon_\\theta(x_t,t)$ starting from $x_T\\sim \\mathcal{N}(0,I)$, iterate for $t=T,\\dots, 1$:\n",
    "\\begin{align*}\n",
    "x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}}\\left( x_t-\\frac{1-\\alpha_t}{\\sqrt{1-\\overline{\\alpha}_t}}\\epsilon_\\theta(x_t,t)\\right)+\\sqrt{\\beta_t}\\epsilon,\\text{ with } \\epsilon\\sim\\mathcal{N}(0,I).\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934767ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Subset\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0282585f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images, title=\"\"):\n",
    "    \"\"\"Shows the provided images as sub-pictures in a square\"\"\"\n",
    "    images = [np.clip(im.permute(1,2,0).numpy(),0,1) for im in images]\n",
    "\n",
    "    # Defining number of rows and columns\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    rows = int(len(images) ** (1 / 2))\n",
    "    cols = round(len(images) / rows)\n",
    "\n",
    "    # Populating figure with sub-plots\n",
    "    idx = 0\n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "            fig.add_subplot(rows, cols, idx + 1)\n",
    "\n",
    "            if idx < len(images):\n",
    "                plt.imshow(images[idx])\n",
    "                plt.axis('off')\n",
    "                idx += 1\n",
    "    fig.suptitle(title, fontsize=30)\n",
    "    \n",
    "    # Showing the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bec9075",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713de2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinusoidal_embedding(n, d):\n",
    "    # Returns the standard positional embedding\n",
    "    embedding = torch.tensor([[i / 10_000 ** (2 * j / d) for j in range(d)] for i in range(n)])\n",
    "    sin_mask = torch.arange(0, n, 2)\n",
    "    embedding[sin_mask] = torch.sin(embedding[sin_mask])\n",
    "    embedding[1 - sin_mask] = torch.cos(embedding[sin_mask])\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885f4d53",
   "metadata": {},
   "source": [
    "## UNet\n",
    "\n",
    "For the $\\epsilon_\\theta(x,t)$ network, we adapt sligthly the UNet architecture used in [UNet_image_seg.ipynb](https://github.com/dataflowr/notebooks/blob/master/Module9/UNet_image_seg.ipynb) by adding an encoding for the time $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc208c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class double_conv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(double_conv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class down_layer(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(down_layer, self).__init__()\n",
    "        self.pool = nn.MaxPool2d(2, stride=2, padding=0)\n",
    "        self.conv = double_conv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(self.pool(x))\n",
    "        return x\n",
    "\n",
    "class up(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(up, self).__init__()\n",
    "        self.up_scale = nn.ConvTranspose2d(in_ch, out_ch, 2, stride=2)\n",
    "\n",
    "    def forward(self, x1, x2): # x1 (bs,out_ch,w1,h1) x2 (bs,in_ch,w2,h2)\n",
    "        x2 = self.up_scale(x2) # (bs,out_ch,2*w2,2*h2)\n",
    "        diffY = x1.size()[2] - x2.size()[2]\n",
    "        diffX = x1.size()[3] - x2.size()[3]\n",
    "\n",
    "        x2 = F.pad(x2, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2]) # (bs,out_ch,w1,h1)\n",
    "        x = torch.cat([x2, x1], dim=1) # (bs,2*out_ch,w1,h1)\n",
    "        return x\n",
    "\n",
    "class up_layer(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch): # !! 2*out_ch = in_ch !!\n",
    "        super(up_layer, self).__init__()\n",
    "        self.up = up(in_ch, out_ch)\n",
    "        self.conv = double_conv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x1, x2): # x1 (bs,out_ch,w1,h1) x2 (bs,in_ch,w2,h2)\n",
    "        a = self.up(x1, x2) # (bs,2*out_ch,w1,h1)\n",
    "        x = self.conv(a) # (bs,out_ch,w1,h1) because 2*out_ch = in_ch\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efeea0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, n_steps=1000, time_emb_dim=100):\n",
    "        super(UNet, self).__init__()\n",
    "        self.conv1 = double_conv(in_channels, 64)\n",
    "        self.down1 = down_layer(64, 128)\n",
    "        self.down2 = down_layer(128, 256)\n",
    "        self.down3 = down_layer(256, 512)\n",
    "        self.down4 = down_layer(512, 1024)\n",
    "        self.up1 = up_layer(1024, 512)\n",
    "        self.up2 = up_layer(512, 256)\n",
    "        self.up3 = up_layer(256, 128)\n",
    "        self.up4 = up_layer(128, 64)\n",
    "        self.last_conv = nn.Conv2d(64, in_channels, 1)\n",
    "        \n",
    "        # Time embedding\n",
    "        self.time_embed = nn.Embedding(n_steps, time_emb_dim)\n",
    "        self.time_embed.weight.data = sinusoidal_embedding(n_steps, time_emb_dim)\n",
    "        self.time_embed.requires_grad_(False)\n",
    "        self.te1 = self._make_te(time_emb_dim, in_channels)\n",
    "        self.te2 = self._make_te(time_emb_dim, 64)\n",
    "        self.te3 = self._make_te(time_emb_dim, 128)\n",
    "        self.te4 = self._make_te(time_emb_dim, 256)\n",
    "        self.te5 = self._make_te(time_emb_dim, 512)\n",
    "        self.te1_up = self._make_te(time_emb_dim, 1024)\n",
    "        self.te2_up = self._make_te(time_emb_dim, 512)\n",
    "        self.te3_up = self._make_te(time_emb_dim, 256)\n",
    "        self.te4_up = self._make_te(time_emb_dim, 128)\n",
    "\n",
    "    def _make_te(self, dim_in, dim_out):\n",
    "        return nn.Sequential(nn.Linear(dim_in, dim_out), nn.SiLU(), nn.Linear(dim_out, dim_out))\n",
    "    \n",
    "    def forward(self, x , t): # x (bs,in_channels,w,d)\n",
    "        bs = x.shape[0]\n",
    "        t = self.time_embed(t)\n",
    "        x1 = self.conv1(x+self.te1(t).reshape(bs, -1, 1, 1)) # (bs,64,w,d)\n",
    "        x2 = self.down1(x1+self.te2(t).reshape(bs, -1, 1, 1)) # (bs,128,w/2,d/2)\n",
    "        x3 = self.down2(x2+self.te3(t).reshape(bs, -1, 1, 1)) # (bs,256,w/4,d/4)\n",
    "        x4 = self.down3(x3+self.te4(t).reshape(bs, -1, 1, 1)) # (bs,512,w/8,h/8)\n",
    "        x5 = self.down4(x4+self.te5(t).reshape(bs, -1, 1, 1)) # (bs,1024,w/16,h/16)\n",
    "        x1_up = self.up1(x4, x5+self.te1_up(t).reshape(bs, -1, 1, 1)) # (bs,512,w/8,h/8)\n",
    "        x2_up = self.up2(x3, x1_up+self.te2_up(t).reshape(bs, -1, 1, 1)) # (bs,256,w/4,h/4)\n",
    "        x3_up = self.up3(x2, x2_up+self.te3_up(t).reshape(bs, -1, 1, 1)) # (bs,128,w/2,h/2)\n",
    "        x4_up = self.up4(x1, x3_up+self.te4_up(t).reshape(bs, -1, 1, 1)) # (bs,64,w,h)\n",
    "        output = self.last_conv(x4_up) # (bs,in_channels,w,h)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71d57f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 3\n",
    "x = torch.randn(bs,1,32,32)\n",
    "n_steps=1000\n",
    "timesteps = torch.randint(0, n_steps, (bs,)).long()\n",
    "unet = UNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1762df",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = unet(x,timesteps)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c37c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPM(nn.Module):\n",
    "    def __init__(self, network, num_timesteps, beta_start=0.0001, beta_end=0.02, device=device) -> None:\n",
    "        super(DDPM, self).__init__()\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.betas = torch.linspace(beta_start, beta_end, num_timesteps, dtype=torch.float32).to(device)\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, axis=0)\n",
    "        self.network = network\n",
    "        self.device = device\n",
    "        self.sqrt_alphas_cumprod = self.alphas_cumprod ** 0.5 # used in add_noise\n",
    "        self.sqrt_one_minus_alphas_cumprod = (1 - self.alphas_cumprod) ** 0.5 # used in add_noise and step\n",
    "\n",
    "    def add_noise(self, x_start, x_noise, timesteps):\n",
    "        # The forward process\n",
    "        # x_start and x_noise (bs, n_c, w, d)\n",
    "        # timesteps (bs)\n",
    "        s1 = self.sqrt_alphas_cumprod[timesteps] # bs\n",
    "        s2 = self.sqrt_one_minus_alphas_cumprod[timesteps] # bs\n",
    "        s1 = s1.reshape(-1,1,1,1) # (bs, 1, 1, 1) for broadcasting\n",
    "        s2 = s2.reshape(-1,1,1,1) # (bs, 1, 1, 1)\n",
    "        return s1 * x_start + s2 * x_noise\n",
    "\n",
    "    def reverse(self, x, t):\n",
    "        # The network return the estimation of the noise we added\n",
    "        return self.network(x, t)\n",
    "    \n",
    "    def step(self, model_output, timestep, sample):\n",
    "        # one step of sampling\n",
    "        # timestep (1)\n",
    "        t = timestep\n",
    "        coef_epsilon = (1-self.alphas)/self.sqrt_one_minus_alphas_cumprod\n",
    "        coef_eps_t = coef_epsilon[t].reshape(-1,1,1,1)\n",
    "        coef_first = 1/self.alphas ** 0.5\n",
    "        coef_first_t = coef_first[t].reshape(-1,1,1,1)\n",
    "        pred_prev_sample = coef_first_t*(sample-coef_eps_t*model_output)\n",
    "\n",
    "        variance = 0\n",
    "        if t > 0:\n",
    "            noise = torch.randn_like(model_output).to(self.device)\n",
    "            variance = ((self.betas[t] ** 0.5) * noise)\n",
    "            \n",
    "        pred_prev_sample = pred_prev_sample + variance\n",
    "\n",
    "        return pred_prev_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4f6222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, dataloader, optimizer, num_epochs, num_timesteps, device=device):\n",
    "    \"\"\"Training loop for DDPM\"\"\"\n",
    "\n",
    "    global_step = 0\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        progress_bar = tqdm(total=len(dataloader))\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "        for step, batch in enumerate(dataloader):\n",
    "            batch = batch[0].to(device)\n",
    "            noise = torch.randn(batch.shape).to(device)\n",
    "            timesteps = torch.randint(0, num_timesteps, (batch.shape[0],)).long().to(device)\n",
    "\n",
    "            noisy = model.add_noise(batch, noise, timesteps)\n",
    "            noise_pred = model.reverse(noisy, timesteps)\n",
    "            loss = F.mse_loss(noise_pred, noise)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            progress_bar.update(1)\n",
    "            logs = {\"loss\": loss.detach().item(), \"step\": global_step}\n",
    "            losses.append(loss.detach().item())\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            global_step += 1\n",
    "        \n",
    "        progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42c4b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = './data/'\n",
    "transforms01 = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        #torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "dataset = torchvision.datasets.CIFAR10(root=root_dir, train=True, transform=transforms01, download=True)\n",
    "dataloader = torch.utils.data.DataLoader(dataset=dataset, batch_size=512, shuffle=True,num_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866e06b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in dataloader:\n",
    "    batch = b[0]\n",
    "    break\n",
    "\n",
    "bn = [b for b in batch[:100]] \n",
    "show_images(bn, \"origin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fec1b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "num_epochs = 15\n",
    "num_timesteps = 1000\n",
    "network = UNet(in_channels=3)\n",
    "network.to(device)\n",
    "model = DDPM(network, num_timesteps, beta_start=0.0001, beta_end=0.02, device=device)\n",
    "model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "training_loop(model, dataloader, optimizer, num_epochs, num_timesteps, device=device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2cd2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image(ddpm, sample_size, channel, size):\n",
    "    \"\"\"Generate the image from the Gaussian noise\"\"\"\n",
    "\n",
    "    frames = []\n",
    "    frames_mid = []\n",
    "    ddpm.eval()\n",
    "    with torch.no_grad():\n",
    "        timesteps = list(range(ddpm.num_timesteps))[::-1]\n",
    "        sample = torch.randn(sample_size, channel, size, size).to(device)\n",
    "        \n",
    "        for i, t in enumerate(tqdm(timesteps)):\n",
    "            time_tensor = (torch.ones(sample_size) * t).long().to(device)\n",
    "            residual = ddpm.reverse(sample, time_tensor).to(device)\n",
    "            sample = ddpm.step(residual, time_tensor[0], sample)\n",
    "\n",
    "            if t==500:\n",
    "                #sample_squeezed = torch.squeeze(sample)\n",
    "                for i in range(sample_size):\n",
    "                    frames_mid.append(sample[i].detach().cpu())\n",
    "\n",
    "        #sample = torch.squeeze(sample)\n",
    "        for i in range(sample_size):\n",
    "            frames.append(sample[i].detach().cpu())\n",
    "    return frames, frames_mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ac7e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated, generated_mid = generate_image(model, 100, 3, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f184f89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(generated_mid, \"Mid result\")\n",
    "show_images(generated, \"Final result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82113e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataloader(dataset, class_name ='ship'):\n",
    "    s_indices = []\n",
    "    s_idx = dataset.class_to_idx[class_name]\n",
    "    for i in range(len(dataset)):\n",
    "        current_class = dataset[i][1]\n",
    "        if current_class == s_idx:\n",
    "            s_indices.append(i)\n",
    "    s_dataset = Subset(dataset, s_indices)\n",
    "    return torch.utils.data.DataLoader(dataset=s_dataset, batch_size=512, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b309a493",
   "metadata": {},
   "outputs": [],
   "source": [
    "ship_dataloader = make_dataloader(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef32b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ship_network = copy.deepcopy(network)\n",
    "ship_model = DDPM(ship_network, num_timesteps, beta_start=0.0001, beta_end=0.02, device=device)\n",
    "num_epochs = 10\n",
    "num_timesteps = model.num_timesteps\n",
    "learning_rate = 1e-3\n",
    "ship_model.train()\n",
    "optimizer = torch.optim.Adam(ship_model.parameters(), lr=learning_rate)\n",
    "training_loop(ship_model, ship_dataloader, optimizer, num_epochs, num_timesteps, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0253facc",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated, generated_mid = generate_image(ship_model, 100, 3, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13a8d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(generated, \"Generated ships\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec3aaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "horse_dataloader = make_dataloader(dataset, 'horse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a701ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "horse_network = copy.deepcopy(network)\n",
    "horse_model = DDPM(horse_network, num_timesteps, beta_start=0.0001, beta_end=0.02, device=device)\n",
    "num_epochs = 10\n",
    "num_timesteps = model.num_timesteps\n",
    "learning_rate = 1e-3\n",
    "horse_model.train()\n",
    "optimizer = torch.optim.Adam(horse_model.parameters(), lr=learning_rate)\n",
    "training_loop(horse_model, horse_dataloader, optimizer, num_epochs, num_timesteps, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0378b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated, generated_mid = generate_image(horse_model, 100, 3, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b595222",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(generated, \"Generated horses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479909b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "truck_dataloader = make_dataloader(dataset, 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1de3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "truck_network = copy.deepcopy(network)\n",
    "truck_model = DDPM(truck_network, num_timesteps, beta_start=0.0001, beta_end=0.02, device=device)\n",
    "num_epochs = 10\n",
    "num_timesteps = model.num_timesteps\n",
    "learning_rate = 1e-3\n",
    "truck_model.train()\n",
    "optimizer = torch.optim.Adam(truck_model.parameters(), lr=learning_rate)\n",
    "training_loop(truck_model, truck_dataloader, optimizer, num_epochs, num_timesteps, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373adc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated, generated_mid = generate_image(truck_model, 100, 3, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d91b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(generated, \"Generated trucks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee66171f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dldiy",
   "language": "python",
   "name": "dldiy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
